{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9d5a6925-4482-4127-ad56-d0650270bb13",
   "metadata": {},
   "source": [
    "# 一、The Multilayer Perceptron（多层感知器）  \n",
    "多层感知器（Multilayer Perceptron，MLP）是一种前向结构的人工神经网络，映射一组输入向量到一组输出向量。\n",
    "在MLP中，信息从输入层经过一系列的隐藏层传递到输出层。每个隐藏层都与下一层全连接，意味着每个节点都与下一层的每个节点相连。   \n",
    "\n",
    "   \n",
    "因此，MLP可以被看做一个有向图，由多个的节点层所组成，每一层都全连接到下一层。在网络中，除输入节点，每个节点都是一个带有非线性激活函数的神经元（或称为处理单元）。\n",
    "\n",
    "MLP 使用一种叫做反向传播的监督学习方法进行训练。其也可以用于分类问题，其中输出通常通过 softmax 函数进行调整以表示概率分布。  \n",
    "Softmax函数将网络的原始输出转换为表示各个类别概率的形式，从而使得网络可以输出对不同类别的概率预测。\n",
    "\n",
    "具体来说，MLP可以分为以下的主要要素：  \n",
    "\n",
    "    \n",
    "1.层数和大小：  \n",
    "MLP由多层节点组成，包括输入层、隐藏层和输出层。隐藏层可以有一个或多个，并且每个隐藏层可以包含不同数量的节点。  \n",
    "输入层的节点数由特征的数量决定，输出层的节点数由任务的类别数量决定。\n",
    "\n",
    "2.权重和偏置：  \n",
    "每个连接都有一个权重，它表示了两个相连节点之间的连接强度。  \n",
    "每个节点（除了输入节点）都有一个偏置，它可以帮助模型更好地拟合数据。\n",
    "\n",
    "3.激活函数：  \n",
    "每个节点都有一个激活函数，它用来引入非线性特性。常用的激活函数包括sigmoid函数、tanh函数、ReLU函数等。  \n",
    "激活函数使得神经网络可以学习非线性关系，从而提高其表达能力。\n",
    "\n",
    "4.反向传播和优化：  \n",
    "MLP通常使用梯度下降优化算法进行训练，其中权重的更新利用反向传播算法来计算损失函数的梯度。  \n",
    "反向传播算法通过链式法则来计算损失函数对于每个参数的梯度，从而实现参数的更新。\n",
    "\n",
    "5.损失函数：  \n",
    "损失函数用来量化模型的预测与实际目标值之间的差异。常见的损失函数包括交叉熵损失函数、平方误差损失函数等。  \n",
    "优化算法的目标是最小化损失函数，以使模型的预测尽可能接近真实的目标值。\n",
    "\n",
    "\n",
    "总的来说，MLP作为一种通用的神经网络结构，具有良好的表达能力，可以应用于各种监督学习任务，特别是在分类问题中有着广泛的应用。  \n",
    "一些适用 MLP 的常见任务包括分类（如垃圾邮件检测，图像分类）和回归（如房价预测）。  \n",
    "\n",
    "![](https://yifdu.github.io/2018/12/20/Natural-Language-Processing-with-PyTorch%EF%BC%88%E5%9B%9B%EF%BC%89/MLP.png)\n",
    "<font color='grey'><center>图1.1 一种具有两个线性层和三个表示阶段（输入向量、隐藏向量和输出向量)的MLP的可视化表示</font></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a553fe2f-5767-45ea-b855-001336ed3c69",
   "metadata": {},
   "source": [
    "# 二、将MLP应用于将姓氏分类到其原籍国  \n",
    "\n",
    "**2.1 任务介绍**  \n",
    "现在，我们将MLP应用于将姓氏分类到其原籍国的任务。这是一个文本分类问题，其中输入是姓氏字符串，输出是姓氏所属的国家或地区。\n",
    "\n",
    "首先，在收集数据前需要明确：  \n",
    "1.在使用人口统计信息时需要遵循公平和合规原则，人口统计信息和其他自我识别信息被统称为“受保护属性”，这表示这些信息可能受到法律保护，因为它们可能涉及个人隐私和歧视等问题。    \n",
    "  \n",
    "2.在使用这些属性时，必须确保结果是公平的。这意味着在产品推荐、服务提供、社会政策等方面，不能因为个人的人口统计信息而导致不公平的对待或歧视。   \n",
    "3. 在建立模型和设计产品时，必须小心处理这些属性。这包括确保在模型训练和产品设计中不会因为这些属性而引入偏见或歧视，以及遵守相关的法律法规。   \n",
    "  \n",
    "之后我们准备好数据集，数据集应包含姓氏及其对应的国家或地区标签并经过预处理，如标准化、向量化等，以便输入到MLP模型中。\n",
    "\n",
    "\n",
    "接着设计模型架构：  \n",
    "输入层：姓氏经过向量化后作为输入。  \n",
    "隐藏层：可以包含多个隐藏层，每个隐藏层包含多个神经元。  \n",
    "输出层：根据数据集中的国家或地区标签数量确定输出层的神经元个数，使用softmax激活函数输出每个类别的概率分布。  \n",
    "\n",
    "进行模型训练：  \n",
    "使用训练数据对MLP模型进行训练，通过反向传播算法更新模型参数。  \n",
    "可以使用交叉熵损失函数来衡量模型输出与真实标签之间的差距。  \n",
    "  \n",
    "最后进行模型评估：  \n",
    "使用验证集对模型进行评估，调整超参数以提高模型性能。  \n",
    "使用测试集评估模型的泛化能力。  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b69094c-8dd8-48aa-b865-c51403256b24",
   "metadata": {},
   "source": [
    "**2.2 姓氏数据集预处理**\n",
    "\n",
    "![](https://p.sda1.dev/17/9c4b45eae874dc0006c5373234a7a8dc/surnames.png)\n",
    "<font color='grey'><center>图2.1 姓氏数据集</font></center>\n",
    "\n",
    "数据集共有10980个样本，来自18个不同的国家。\n",
    "我们首先从文件 \"surnames.csv\" 中读取姓氏数据（这个文件应该包含姓氏及其对应的国籍等信息），通过 pandas 的 read_csv 函数，将数据读入 DataFrame（表格结构）中。\n",
    "\n",
    "然后，使用 DataFrame.head() 方法来查看数据集的前五行，以了解数据的结构和内容。\n",
    "\n",
    "最后 set(surnames.nationality) 用于返回姓氏数据集中所有不同的国籍类别。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cf16765c-cd82-44dc-badf-8ddf2535a05e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>surname</th>\n",
       "      <th>nationality</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Woodford</td>\n",
       "      <td>English</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Coté</td>\n",
       "      <td>French</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Kore</td>\n",
       "      <td>English</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Koury</td>\n",
       "      <td>Arabic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Lebzak</td>\n",
       "      <td>Russian</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    surname nationality\n",
       "0  Woodford     English\n",
       "1      Coté      French\n",
       "2      Kore     English\n",
       "3     Koury      Arabic\n",
       "4    Lebzak     Russian"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import collections\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from argparse import Namespace\n",
    "\n",
    "args = Namespace(  # 创建一个命名空间对象，存储所需的参数\n",
    "    raw_dataset_csv=\"surnames.csv\",  # 原始数据集的文件名\n",
    "    train_proportion=0.7,  # 训练集所占比例\n",
    "    val_proportion=0.15,  # 验证集所占比例\n",
    "    test_proportion=0.15,  # 测试集所占比例\n",
    "    output_munged_csv=\"surnames_with_splits.csv\",  # 处理后的数据集要保存的文件名\n",
    "    seed=1337  # 随机数种子，用于保证结果的可重复性\n",
    ")\n",
    "surnames = pd.read_csv(args.raw_dataset_csv, header=0)\n",
    "# 显示数据集前5行\n",
    "surnames.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "393173ce-29fe-4833-9759-9efb7aba0195",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Arabic',\n",
       " 'Chinese',\n",
       " 'Czech',\n",
       " 'Dutch',\n",
       " 'English',\n",
       " 'French',\n",
       " 'German',\n",
       " 'Greek',\n",
       " 'Irish',\n",
       " 'Italian',\n",
       " 'Japanese',\n",
       " 'Korean',\n",
       " 'Polish',\n",
       " 'Portuguese',\n",
       " 'Russian',\n",
       " 'Scottish',\n",
       " 'Spanish',\n",
       " 'Vietnamese'}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 查看样本的nationality(国籍种类)\n",
    "set(surnames.nationality)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f2d8f8b-f7c7-4eaf-89f8-0e96589c07e4",
   "metadata": {},
   "source": [
    "       \n",
    "接下来，我们根据国籍对数据集进行分组，并将数据集分为三个部分:  \n",
    "70%到训练数据集，15%到验证数据集，最后15%到测试数据集，以便跨这些部分的类标签分布具有可比性。\n",
    "             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "22275dcc-49f7-4cae-9aa2-7531a16eea15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建一个默认字典，键对应国籍，值是对应国籍下所有姓氏数据的字典列表\n",
    "by_nationality = collections.defaultdict(list)\n",
    "\n",
    "# 遍历姓氏数据集的每一行\n",
    "for _, row in surnames.iterrows():\n",
    "    # 将每一行的数据添加到与其国籍对应的列表中\n",
    "    by_nationality[row.nationality].append(row.to_dict())\n",
    "\n",
    "final_list = []  # 创建一个空列表，用于存储最终的数据\n",
    "\n",
    "np.random.seed(args.seed)  # 设置随机数种子，保证结果的可重复性\n",
    "\n",
    "# 对字典按照国籍进行排序，并遍历所有键值对\n",
    "for _, item_list in sorted(by_nationality.items()):\n",
    "    np.random.shuffle(item_list)  # 随机打乱每个国籍对应的姓氏数据列表\n",
    "    n = len(item_list)  # 获取该国籍下的姓氏数据数量\n",
    "    n_train = int(args.train_proportion * n)  # 计算训练集数量\n",
    "    n_val = int(args.val_proportion * n)  # 计算验证集数量\n",
    "    n_test = int(args.test_proportion * n)  # 计算测试集数量\n",
    "    \n",
    "    # 将数据按比例分成训练集、验证集和测试集，并为每条数据添加一个 'split' 标签\n",
    "    for item in item_list[:n_train]:\n",
    "        item['split'] = 'train'  # 标记为训练集\n",
    "    for item in item_list[n_train:n_train + n_val]:\n",
    "        item['split'] = 'val'  # 标记为验证集\n",
    "    for item in item_list[n_train + n_val:]:\n",
    "        item['split'] = 'test'  # 标记为测试集\n",
    "    \n",
    "    final_list.extend(item_list)  # 将处理后的数据添加到最终数据列表中\n",
    "\n",
    "# 将最终的数据列表转换为 pandas DataFrame\n",
    "final_surnames = pd.DataFrame(final_list)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "411cfbcc-97a9-42b8-94b4-d2158d00599b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "split\n",
       "train    7680\n",
       "test     1660\n",
       "val      1640\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#显示spilt标签中不同值出现的次数,即获得训练集，测试集，验证集的数量\n",
    "final_surnames.split.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "640f79bd-1d1b-443d-a696-11f3ae7dce7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>surname</th>\n",
       "      <th>nationality</th>\n",
       "      <th>split</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Totah</td>\n",
       "      <td>Arabic</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Abboud</td>\n",
       "      <td>Arabic</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Fakhoury</td>\n",
       "      <td>Arabic</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Srour</td>\n",
       "      <td>Arabic</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Sayegh</td>\n",
       "      <td>Arabic</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    surname nationality  split\n",
       "0     Totah      Arabic  train\n",
       "1    Abboud      Arabic  train\n",
       "2  Fakhoury      Arabic  train\n",
       "3     Srour      Arabic  train\n",
       "4    Sayegh      Arabic  train"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 显示处理后的数据集前5行\n",
    "final_surnames.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "720705ad-c97c-4039-8b22-564dbe888c06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将最终的数据保存为 CSV 文件\n",
    "final_surnames.to_csv(args.output_munged_csv, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87586c1a-a878-407e-9a94-6d4ccc7b0b67",
   "metadata": {},
   "source": [
    "**2.2.1  The Surname Dataset**  \n",
    "SurnameDataset数据集类继承自PyTorch的数据集类，其中我们先实现了两个函数：  \n",
    "__getitem__方法返回给定索引所对应的数据点(向量化的姓氏和与其国籍相对应的索引)  \n",
    "__len__方法返回数据集的长度  \n",
    "同时实现其他函数便于我们加载或处理数据。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c362dd70-558c-4908-ae94-333b5fa7e96e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class SurnameDataset(Dataset):\n",
    "    def __init__(self, surname_df, vectorizer):\n",
    "        \"\"\"\n",
    "        初始化SurnameDataset类\n",
    "\n",
    "        Args:\n",
    "            surname_df (pandas.DataFrame): 数据集\n",
    "            vectorizer (SurnameVectorizer): 从数据集实例化的向量化器\n",
    "        \"\"\"\n",
    "        self.surname_df = surname_df\n",
    "        self._vectorizer = vectorizer\n",
    "\n",
    "        # 划分数据集\n",
    "        self.train_df = self.surname_df[self.surname_df.split == 'train']\n",
    "        self.train_size = len(self.train_df)\n",
    "        self.val_df = self.surname_df[self.surname_df.split == 'val']\n",
    "        self.validation_size = len(self.val_df)\n",
    "        self.test_df = self.surname_df[self.surname_df.split == 'test']\n",
    "        self.test_size = len(self.test_df)\n",
    "        self._lookup_dict = {'train': (self.train_df, self.train_size),\n",
    "                             'val': (self.val_df, self.validation_size),\n",
    "                             'test': (self.test_df, self.test_size)}\n",
    "        self.set_split('train')\n",
    "\n",
    "        # 计算类别权重\n",
    "        class_counts = surname_df.nationality.value_counts().to_dict()\n",
    "\n",
    "        def sort_key(item):\n",
    "            return self._vectorizer.nationality_vocab.lookup_token(item[0])\n",
    "\n",
    "        sorted_counts = sorted(class_counts.items(), key=sort_key)\n",
    "        frequencies = [count for _, count in sorted_counts]\n",
    "        self.class_weights = 1.0 / torch.tensor(frequencies, dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"返回数据集的长度\"\"\"\n",
    "        return self._target_size\n",
    "        \n",
    "    def get_num_batches(self, batch_size):\n",
    "        \"\"\"给定批量大小，返回数据集中的批量数\n",
    "\n",
    "        Args:\n",
    "            batch_size (int)\n",
    "        Returns:\n",
    "            数据集中的批量数\n",
    "        \"\"\"\n",
    "        return len(self) // batch_size\n",
    "\n",
    "    @classmethod\n",
    "    def load_dataset_and_make_vectorizer(cls, surname_csv):\n",
    "        \"\"\"加载数据集并从头开始创建一个新的向量化器\n",
    "\n",
    "        Args:\n",
    "            surname_csv (str): 数据集的位置\n",
    "        Returns:\n",
    "            SurnameDataset的一个实例\n",
    "        \"\"\"\n",
    "        surname_df = pd.read_csv(surname_csv)\n",
    "        train_surname_df = surname_df[surname_df.split == 'train']\n",
    "        return cls(surname_df, SurnameVectorizer.from_dataframe(train_surname_df))\n",
    "\n",
    "    @classmethod\n",
    "    def load_dataset_and_load_vectorizer(cls, surname_csv, vectorizer_filepath):\n",
    "        \"\"\"加载数据集和相应的向量化器。用于当向量化器已被缓存以便重复使用时\n",
    "\n",
    "        Args:\n",
    "            surname_csv (str): 数据集的位置\n",
    "            vectorizer_filepath (str): 保存的向量化器的位置\n",
    "        Returns:\n",
    "            SurnameDataset的一个实例\n",
    "        \"\"\"\n",
    "        surname_df = pd.read_csv(surname_csv)\n",
    "        vectorizer = cls.load_vectorizer_only(vectorizer_filepath)\n",
    "        return cls(surname_df, vectorizer)\n",
    "\n",
    "    @staticmethod\n",
    "    def load_vectorizer_only(vectorizer_filepath):\n",
    "        \"\"\"从文件中加载向量化器的静态方法\n",
    "\n",
    "        Args:\n",
    "            vectorizer_filepath (str): 序列化向量化器的位置\n",
    "        Returns:\n",
    "            SurnameVectorizer的一个实例\n",
    "        \"\"\"\n",
    "        with open(vectorizer_filepath) as fp:\n",
    "            return SurnameVectorizer.from_serializable(json.load(fp))\n",
    "\n",
    "    def save_vectorizer(self, vectorizer_filepath):\n",
    "        \"\"\"使用json将向量化器保存到磁盘\n",
    "\n",
    "        Args:\n",
    "            vectorizer_filepath (str): 保存向量化器的位置\n",
    "        \"\"\"\n",
    "        with open(vectorizer_filepath, \"w\") as fp:\n",
    "            json.dump(self._vectorizer.to_serializable(), fp)\n",
    "\n",
    "    def get_vectorizer(self):\n",
    "        \"\"\"返回向量化器\"\"\"\n",
    "        return self._vectorizer\n",
    "\n",
    "    def set_split(self, split=\"train\"):\n",
    "        \"\"\"使用数据框中的列选择数据集中的拆分部分\"\"\"\n",
    "        self._target_split = split\n",
    "        self._target_df, self._target_size = self._lookup_dict[split]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"PyTorch数据集的主要入口方法\n",
    "        Args:\n",
    "            index (int): 数据点的索引\n",
    "        Returns:\n",
    "            一个包含数据点的字典（字典值分别是向量化的姓氏和与其国籍相对应的索引）:\n",
    "                feature (x_surname)\n",
    "                label (y_nationality)\n",
    "        \"\"\"\n",
    "        row = self._target_df.iloc[index]\n",
    "        surname_vector = \\\n",
    "            self._vectorizer.vectorize(row.surname)\n",
    "        nationality_index = \\\n",
    "            self._vectorizer.nationality_vocab.lookup_token(row.nationality)\n",
    "        return {'x_surname': surname_vector,\n",
    "                'y_nationality': nationality_index}\n",
    "\n",
    "\n",
    "def generate_batches(dataset, batch_size, shuffle=True,\n",
    "                     drop_last=True, device=\"cpu\"):\n",
    "    \"\"\"\n",
    "    一个生成器函数，用于包装PyTorch DataLoader。它将确保每个张量位于正确的设备位置。\n",
    "    \"\"\"\n",
    "    dataloader = DataLoader(dataset=dataset, batch_size=batch_size,\n",
    "                            shuffle=shuffle, drop_last=drop_last)\n",
    "\n",
    "    for data_dict in dataloader:\n",
    "        out_data_dict = {}\n",
    "        for name, tensor in data_dict.items():\n",
    "            out_data_dict[name] = data_dict[name].to(device)\n",
    "        yield out_data_dict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4c95f0a-8b61-4384-a60e-e7245560d732",
   "metadata": {},
   "source": [
    "**2.2.2 Vocabulary, Vectorizer, and DataLoader**    \n",
    "为了使用字符对姓氏进行分类，我们使用词汇表、向量化器和DataLoader将姓氏字符串转换为向量化的minibatches。\n",
    "\n",
    "词汇表类（Vocabulary Class）：  \n",
    "词汇表类由两个Python字典组成，一个用于将字符映射到整数索引，另一个用于将整数索引映射回字符。这种双向映射使得我们可以根据需要在字符和整数索引之间进行转换。\n",
    "使用的是one-hot词汇表，不计算字符出现的频率，只对频繁出现的条目进行限制。  \n",
    "\n",
    "姓氏向量化器（SurnameVectorizer）：  \n",
    "该向量化器负责将姓氏应用于词汇表，并将其转换为向量表示。  \n",
    "姓氏是字符的序列，每个字符在词汇表中是一个独立的标记。  \n",
    "在此处，我们将暂时忽略字符序列信息，通过迭代字符串输入中的每个字符来创建输入的压缩one-hot向量表示。  \n",
    "对于以前未遇到的字符，指定一个特殊的令牌UNK（未知字符）。在实例化词汇表时，仅从训练数据中构建词汇表，验证或测试数据中可能存在未知字符，因此在字符词汇表中仍然使用UNK符号。  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "31d35b47-2e28-4570-8946-1b66a8dbd020",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocabulary(object):\n",
    "    \"\"\"用于处理文本并提取词汇以进行映射的类\"\"\"\n",
    "\n",
    "    def __init__(self, token_to_idx=None, add_unk=True, unk_token=\"<UNK>\"):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            token_to_idx (dict): 一个已存在的将标记映射到索引的字典\n",
    "            add_unk (bool): 一个指示是否添加UNK标记的标志\n",
    "            unk_token (str): 要添加到词汇表中的UNK标记\n",
    "        \"\"\"\n",
    "\n",
    "        if token_to_idx is None:\n",
    "            token_to_idx = {}\n",
    "        self._token_to_idx = token_to_idx\n",
    "\n",
    "        self._idx_to_token = {idx: token for token, idx in self._token_to_idx.items()}\n",
    "\n",
    "        self._add_unk = add_unk\n",
    "        self._unk_token = unk_token\n",
    "\n",
    "        self.unk_index = -1\n",
    "        if add_unk:\n",
    "            self.unk_index = self.add_token(unk_token)\n",
    "\n",
    "    def to_serializable(self):\n",
    "        \"\"\"返回一个可序列化的字典\"\"\"\n",
    "        return {'token_to_idx': self._token_to_idx,\n",
    "                'add_unk': self._add_unk,\n",
    "                'unk_token': self._unk_token}\n",
    "\n",
    "    @classmethod\n",
    "    def from_serializable(cls, contents):\n",
    "        \"\"\"从序列化的字典实例化Vocabulary\"\"\"\n",
    "        return cls(**contents)\n",
    "\n",
    "    def add_token(self, token):\n",
    "        \"\"\"根据标记更新映射字典。\n",
    "\n",
    "        Args:\n",
    "            token (str): 要添加到词汇表中的项目\n",
    "        Returns:\n",
    "            index (int): 与标记对应的整数\n",
    "        \"\"\"\n",
    "        try:\n",
    "            index = self._token_to_idx[token]\n",
    "        except KeyError:\n",
    "            index = len(self._token_to_idx)\n",
    "            self._token_to_idx[token] = index\n",
    "            self._idx_to_token[index] = token\n",
    "        return index\n",
    "\n",
    "    def add_many(self, tokens):\n",
    "        \"\"\"将标记列表添加到词汇表中\n",
    "\n",
    "        Args:\n",
    "            tokens (list): 一个字符串标记列表\n",
    "        Returns:\n",
    "            indices (list): 与标记对应的索引列表\n",
    "        \"\"\"\n",
    "        return [self.add_token(token) for token in tokens]\n",
    "\n",
    "    def lookup_token(self, token):\n",
    "        \"\"\"检索与标记关联的索引，如果标记不存在，则返回UNK索引。\n",
    "\n",
    "        Args:\n",
    "            token (str): 要查找的标记\n",
    "        Returns:\n",
    "            index (int): 与标记对应的索引\n",
    "        Notes:\n",
    "            `unk_index` 需要 >=0（已添加到词汇表中）以实现UNK功能\n",
    "        \"\"\"\n",
    "        if self.unk_index >= 0:\n",
    "            return self._token_to_idx.get(token, self.unk_index)\n",
    "        else:\n",
    "            return self._token_to_idx[token]\n",
    "\n",
    "    def lookup_index(self, index):\n",
    "        \"\"\"返回与索引关联的标记\n",
    "\n",
    "        Args:\n",
    "            index (int): 要查找的索引\n",
    "        Returns:\n",
    "            token (str): 与索引对应的标记\n",
    "        Raises:\n",
    "            KeyError: 如果索引不在词汇表中\n",
    "        \"\"\"\n",
    "        if index not in self._idx_to_token:\n",
    "            raise KeyError(\"the index (%d) is not in the Vocabulary\" % index)\n",
    "        return self._idx_to_token[index]\n",
    "\n",
    "    def __str__(self):\n",
    "        return \"<Vocabulary(size=%d)>\" % len(self)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._token_to_idx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "437fb8ae-ac6c-4cad-a218-c48b8b7512c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SurnameVectorizer(object):\n",
    "    \"\"\"协调词汇表并将其应用于矢量化的矢量化器\"\"\"\n",
    "\n",
    "    def __init__(self, surname_vocab, nationality_vocab):\n",
    "        self.surname_vocab = surname_vocab\n",
    "        self.nationality_vocab = nationality_vocab\n",
    "\n",
    "    def vectorize(self, surname):\n",
    "        \"\"\"对提供的姓氏进行矢量化\n",
    "\n",
    "        Args:\n",
    "            surname (str): 姓氏\n",
    "        Returns:\n",
    "            one_hot (np.ndarray): 折叠的 one-hot 编码\n",
    "        \"\"\"\n",
    "        vocab = self.surname_vocab\n",
    "        one_hot = np.zeros(len(vocab), dtype=np.float32)\n",
    "        for token in surname:\n",
    "            one_hot[vocab.lookup_token(token)] = 1\n",
    "        return one_hot\n",
    "\n",
    "    @classmethod\n",
    "    def from_dataframe(cls, surname_df):\n",
    "        \"\"\"从数据框实例化矢量化器\n",
    "\n",
    "        Args:\n",
    "            surname_df (pandas.DataFrame): 姓氏数据集\n",
    "        Returns:\n",
    "            SurnameVectorizer 的一个实例\n",
    "        \"\"\"\n",
    "        surname_vocab = Vocabulary(unk_token=\"@\")\n",
    "        nationality_vocab = Vocabulary(add_unk=False)\n",
    "\n",
    "        for index, row in surname_df.iterrows():\n",
    "            for letter in row.surname:\n",
    "                surname_vocab.add_token(letter)\n",
    "            nationality_vocab.add_token(row.nationality)\n",
    "\n",
    "        return cls(surname_vocab, nationality_vocab)\n",
    "\n",
    "    \n",
    "    @classmethod\n",
    "    def from_serializable(cls, contents):\n",
    "        \"\"\"\n",
    "       从可序列化内容实例化一个SurnameVectorizer对象\n",
    "\n",
    "       Args:\n",
    "           contents (dict): 包含可序列化内容的字典\n",
    "\n",
    "       Returns:\n",
    "           SurnameVectorizer的一个实例\n",
    "       \"\"\"\n",
    "        surname_vocab = Vocabulary.from_serializable(contents['surname_vocab'])\n",
    "        nationality_vocab = Vocabulary.from_serializable(contents['nationality_vocab'])\n",
    "        return cls(surname_vocab=surname_vocab, nationality_vocab=nationality_vocab)\n",
    "\n",
    "    \n",
    "    def to_serializable(self):\n",
    "        \"\"\"\n",
    "        将SurnameVectorizer对象序列化为字典\n",
    "        Returns:\n",
    "            dict: 包含序列化内容的字典\n",
    "        \"\"\"\n",
    "        return {'surname_vocab': self.surname_vocab.to_serializable(),\n",
    "            'nationality_vocab': self.nationality_vocab.to_serializable()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1865453a-daa7-4e5e-af7b-9b95456223af",
   "metadata": {},
   "source": [
    "**2.3 实现MLP**   \n",
    "**2.3.1 The Surname Classifier Model**     \n",
    "\n",
    "  \n",
    "设计两层的多层感知器，用于对姓氏进行分类：  \n",
    "1.在前向传播中，输入向量首先经过第一个线性层，将输入向量映射到中间向量，并应用非线性激活函数(ReLU)。  \n",
    "2.然后其结果再经过第二个线性层将中间向量映射到预测向量。  \n",
    "3.最后，可选地应用softmax操作，以确保输出和为1（输出是概率）。 \n",
    "  \n",
    "是否应用softmax操作通过 apply_softmax 参数来控制的。如果 apply_softmax 为 True，则会对预测向量应用 softmax 操作；如果为 False，则不会应用 softmax 操作，这种情况一般出现在与交叉熵损失一起使用时。  \n",
    "\n",
    "  \n",
    "softmax可选的原因在于我们将应用的交叉熵损失函数：  \n",
    "交叉熵损失函数的数学形式要求输入是未经过 softmax 操作的原始预测值，而不是概率分布。因此，当我们使用交叉熵损失函数时，不需要在模型的前向传播中应用 softmax 操作，因为交叉熵损失函数本身会在内部进行softmax操作，此时应用会得到错误的损失值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a9ad7306-a7f7-40be-becc-95d5390ba7a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SurnameClassifier(nn.Module):\n",
    "    \"\"\"用于对姓氏进行分类的两层多层感知器\"\"\"\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            input_dim (int): 输入向量的大小\n",
    "            hidden_dim (int): 第一个线性层的输出大小\n",
    "            output_dim (int): 第二个线性层的输出大小\n",
    "        \"\"\"\n",
    "        super(SurnameClassifier, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x_in, apply_softmax=False):\n",
    "        \"\"\"分类器的前向传播\n",
    "\n",
    "        Args:\n",
    "            x_in (torch.Tensor): 输入数据张量。\n",
    "                x_in.shape 应为 (batch, input_dim)\n",
    "            apply_softmax (bool): 是否应用 softmax 激活函数的标志\n",
    "                如果与交叉熵损失一起使用，应为 False\n",
    "        Returns:\n",
    "            结果张量。张量形状应为 (batch, output_dim)\n",
    "        \"\"\"\n",
    "        intermediate_vector = F.relu(self.fc1(x_in))\n",
    "        prediction_vector = self.fc2(intermediate_vector)\n",
    "\n",
    "        if apply_softmax:\n",
    "            prediction_vector = F.softmax(prediction_vector, dim=1)\n",
    "\n",
    "        return prediction_vector\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "163b855b-177e-4919-98a8-3d649284b178",
   "metadata": {},
   "source": [
    "**2.3.2 The Training Routine**      \n",
    "**2.3.2.1 准备工作**    \n",
    "为了进行模型训练，我们先做好准备工作，包括定义数据和路径信息，设置模型超参数，确保运行环境等等\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "58d59e16-ac5b-4ebe-88f7-23bc6ffac158",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expanded filepaths: \n",
      "\tmodel_storage/ch4/surname_mlp\\vectorizer.json\n",
      "\tmodel_storage/ch4/surname_mlp\\model.pth\n",
      "Using CUDA: False\n",
      "Creating fresh!\n"
     ]
    }
   ],
   "source": [
    "from argparse import Namespace\n",
    "import torch.optim as optim\n",
    "import os\n",
    "import json\n",
    "\n",
    "\n",
    "def set_seed_everywhere(seed, cuda):\n",
    "    \"\"\"\n",
    "    设置随机种子以确保实验的可重复性\n",
    "    \"\"\"\n",
    "    np.random.seed(seed)  # 设置NumPy的随机种子\n",
    "    torch.manual_seed(seed)  # 设置PyTorch的随机种子\n",
    "    if cuda:\n",
    "        torch.cuda.manual_seed_all(seed)  # 如果使用CUDA，设置所有GPU的随机种子\n",
    "\n",
    "def handle_dirs(dirpath):\n",
    "    \"\"\"\n",
    "    处理目录，确保目录存在，如果不存在则创建该目录\n",
    "    \"\"\"\n",
    "    if not os.path.exists(dirpath):  # 如果目录不存在\n",
    "        os.makedirs(dirpath)  # 创建该目录\n",
    "\n",
    "        \n",
    "args = Namespace(\n",
    "    # 数据和路径信息\n",
    "    surname_csv=\"surnames_with_splits.csv\",  # 包含姓氏数据的CSV文件路径\n",
    "    vectorizer_file=\"vectorizer.json\",  # 向量化器文件的路径\n",
    "    model_state_file=\"model.pth\",  # 模型状态文件的路径\n",
    "    save_dir=\"model_storage/ch4/surname_mlp\",  # 模型保存目录\n",
    "    # 模型超参数\n",
    "    hidden_dim=300,  # 隐藏层维度\n",
    "    # 训练超参数\n",
    "    seed=1337,  # 随机种子\n",
    "    num_epochs=100,  # 训练的迭代次数\n",
    "    early_stopping_criteria=5,  # 提前停止的标准\n",
    "    learning_rate=0.001,  # 学习率\n",
    "    batch_size=64,  # 批处理大小\n",
    "    cuda=False,  # 是否使用CUDA\n",
    "    reload_from_files=False,  # 是否从文件重新加载模型\n",
    "    expand_filepaths_to_save_dir=True,  # 是否扩展文件路径到保存目录\n",
    ")\n",
    "\n",
    "if args.expand_filepaths_to_save_dir:\n",
    "    # 如果设置为True，将向量化器文件和模型状态文件的路径扩展到保存目录中\n",
    "    args.vectorizer_file = os.path.join(args.save_dir, args.vectorizer_file)\n",
    "    args.model_state_file = os.path.join(args.save_dir, args.model_state_file)\n",
    "    \n",
    "    print(\"Expanded filepaths: \")\n",
    "    print(\"\\t{}\".format(args.vectorizer_file))  \n",
    "    print(\"\\t{}\".format(args.model_state_file)) \n",
    "    \n",
    "# 检查CUDA是否可用\n",
    "if not torch.cuda.is_available():\n",
    "    args.cuda = False  \n",
    "\n",
    "# 根据args.cuda的值选择设备为CUDA（如果可用）或CPU\n",
    "args.device = torch.device(\"cuda\" if args.cuda else \"cpu\")\n",
    "print(\"Using CUDA: {}\".format(args.cuda))\n",
    "\n",
    "# 为了确保实验的可重复性，设置随机种子\n",
    "set_seed_everywhere(args.seed, args.cuda)\n",
    "\n",
    "# 处理保存目录，确保目录存在\n",
    "handle_dirs(args.save_dir)\n",
    "\n",
    "if args.reload_from_files:\n",
    "    # 从文件中重新加载数据集和向量化器\n",
    "    print(\"Reloading!\")\n",
    "    dataset = SurnameDataset.load_dataset_and_load_vectorizer(args.surname_csv, args.vectorizer_file)\n",
    "else:\n",
    "    # 创建新的数据集和向量化器\n",
    "    print(\"Creating fresh!\")\n",
    "    dataset = SurnameDataset.load_dataset_and_make_vectorizer(args.surname_csv)\n",
    "    dataset.save_vectorizer(args.vectorizer_file)  # 保存向量化器到文件中\n",
    "\n",
    "vectorizer = dataset.get_vectorizer()  # 获取数据集的向量化器\n",
    "\n",
    "# 创建姓氏分类器，设置输入维度、隐藏层维度和输出维度，并将其移动到指定的设备上\n",
    "classifier = SurnameClassifier(input_dim=len(vectorizer.surname_vocab), \n",
    "                               hidden_dim=args.hidden_dim, \n",
    "                               output_dim=len(vectorizer.nationality_vocab))\n",
    "\n",
    "classifier = classifier.to(args.device)  # 将分类器移动到指定的设备上\n",
    "dataset.class_weights = dataset.class_weights.to(args.device) # 将数据集的类别权重移动到指定的设备上"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a9b4ee8-5f2b-43e8-869a-ce63b19248cb",
   "metadata": {},
   "source": [
    "**2.3.2.2 进行模型训练**    \n",
    "接下来我们进行模型训练，在训练中使用的损失函数是CrossEntropyLoss。  \n",
    "它结合了softmax激活函数和交叉熵损失，适用于将模型输出的原始分数转换为概率分布，并计算预测概率分布与实际标签之间的交叉熵损失。  \n",
    "将类别权重作为参数传递给CrossEntropyLoss损失函数，这样就能够根据数据集中类别的不平衡情况来调整损失函数的权重，以更好地处理不同类别之间的影响。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ec8b57f6-524e-4742-a777-12d310a4f135",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用交叉熵损失函数，传入数据集的类别权重\n",
    "loss_func = nn.CrossEntropyLoss(weight=dataset.class_weights)  \n",
    "\n",
    "# 使用Adam优化器，设置学习率为预定义的参数\n",
    "optimizer = optim.Adam(classifier.parameters(), lr=args.learning_rate)  \n",
    "\n",
    "# 设置学习率调度器，当验证损失不再减少时，将学习率缩小一半\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer=optimizer,\n",
    "                                                 mode='min', factor=0.5,\n",
    "                                                 patience=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffeca8ed-78ca-4f63-9244-98f1ff41c513",
   "metadata": {},
   "source": [
    "利用训练数据，计算模型输出、损失和梯度。然后，使用梯度来更新模型。以下是训练中需要用到的相关函数："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "129d21f0-a908-4ec1-b704-66c973517a27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_train_state(args):\n",
    "    \"\"\"\n",
    "    创建一个表示训练状态的字典，用于跟踪训练过程中的各种值和参数\n",
    "\n",
    "    Args:\n",
    "    - args (Namespace): 包含训练参数的命名空间对象\n",
    "\n",
    "    Returns:\n",
    "    - train_state (dict): 表示训练状态的字典\n",
    "    \"\"\"\n",
    "    return {'stop_early': False,  # 是否提前停止训练\n",
    "            'early_stopping_step': 0,  # 提前停止的步数\n",
    "            'early_stopping_best_val': 1e8,  # 最佳验证集损失\n",
    "            'learning_rate': args.learning_rate,  # 学习率\n",
    "            'epoch_index': 0,  # 当前迭代次数\n",
    "            'train_loss': [],  # 训练集损失\n",
    "            'train_acc': [],  # 训练集准确率\n",
    "            'val_loss': [],  # 验证集损失\n",
    "            'val_acc': [],  # 验证集准确率\n",
    "            'test_loss': -1,  # 测试集损失\n",
    "            'test_acc': -1,  # 测试集准确率\n",
    "            'model_filename': args.model_state_file}  # 模型文件名\n",
    "\n",
    "def update_train_state(args, model, train_state):\n",
    "    \"\"\"\n",
    "    处理训练状态的更新，包括提前停止和模型保存\n",
    "\n",
    "    Args:\n",
    "    - args (Namespace): 包含训练参数的命名空间对象\n",
    "    - model: 待训练的模型\n",
    "    - train_state (dict): 表示训练状态的字典\n",
    "\n",
    "    Returns:\n",
    "    - train_state (dict): 更新后的训练状态字典\n",
    "    \"\"\"\n",
    "    # 保存至少一个模型\n",
    "    if train_state['epoch_index'] == 0:\n",
    "        torch.save(model.state_dict(), train_state['model_filename'])\n",
    "        train_state['stop_early'] = False\n",
    "\n",
    "    # 如果训练过至少一个epoch\n",
    "    elif train_state['epoch_index'] >= 1:\n",
    "        loss_tm1, loss_t = train_state['val_loss'][-2:]\n",
    "\n",
    "        # 如果验证集损失变大\n",
    "        if loss_t >= train_state['early_stopping_best_val']:\n",
    "            # 更新提前停止步数\n",
    "            train_state['early_stopping_step'] += 1\n",
    "        # 如果损失减小\n",
    "        else:\n",
    "            # 保存最佳模型\n",
    "            if loss_t < train_state['early_stopping_best_val']:\n",
    "                torch.save(model.state_dict(), train_state['model_filename'])\n",
    "\n",
    "            # 重置提前停止步数\n",
    "            train_state['early_stopping_step'] = 0\n",
    "\n",
    "        # 是否提前停止\n",
    "        train_state['stop_early'] = \\\n",
    "            train_state['early_stopping_step'] >= args.early_stopping_criteria\n",
    "\n",
    "    return train_state\n",
    "\n",
    "def compute_accuracy(y_pred, y_target):\n",
    "    \"\"\"\n",
    "    计算模型的准确率\n",
    "\n",
    "    Args:\n",
    "    - y_pred: 模型的预测结果\n",
    "    - y_target: 真实标签\n",
    "\n",
    "    Returns:\n",
    "    - accuracy (float): 准确率\n",
    "    \"\"\"\n",
    "    _, y_pred_indices = y_pred.max(dim=1)\n",
    "    n_correct = torch.eq(y_pred_indices, y_target).sum().item()\n",
    "    return n_correct / len(y_pred_indices) * 100\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74f80cc4-7d1b-4c9f-829c-226a63ca1395",
   "metadata": {},
   "source": [
    "使用tqdm库可视化训练过程，包括显示进度条、损失值和准确率等指标的实时更新。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1a9ffaab-1260-4255-a466-0236c56c2f31",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\31216\\AppData\\Local\\Temp\\ipykernel_25992\\3173897158.py:7: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  epoch_bar = tqdm_notebook(desc='training routine',\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa57e73568334639a85bb753dea67a85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "training routine:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\31216\\AppData\\Local\\Temp\\ipykernel_25992\\3173897158.py:12: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  train_bar = tqdm_notebook(desc='split=train',\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13679d5ed2d246b78324d08652f9e0e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "split=train:   0%|          | 0/120 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\31216\\AppData\\Local\\Temp\\ipykernel_25992\\3173897158.py:18: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  val_bar = tqdm_notebook(desc='split=val',\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "741b97d83a854c98a3e03d578c476d27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "split=val:   0%|          | 0/120 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tqdm import tqdm_notebook\n",
    "\n",
    "# 初始化训练状态\n",
    "train_state = make_train_state(args)\n",
    "\n",
    "# 创建进度条\n",
    "epoch_bar = tqdm_notebook(desc='training routine', \n",
    "                          total=args.num_epochs,\n",
    "                          position=0)\n",
    "\n",
    "# 设置训练集进度条\n",
    "train_bar = tqdm_notebook(desc='split=train',\n",
    "                          total=dataset.get_num_batches(args.batch_size), \n",
    "                          position=1, \n",
    "                          leave=True)\n",
    "\n",
    "# 设置验证集进度条\n",
    "val_bar = tqdm_notebook(desc='split=val',\n",
    "                        total=dataset.get_num_batches(args.batch_size), \n",
    "                        position=1, \n",
    "                        leave=True)\n",
    "\n",
    "try:\n",
    "    for epoch_index in range(args.num_epochs):\n",
    "        train_state['epoch_index'] = epoch_index\n",
    "\n",
    "        # 迭代训练集\n",
    "        dataset.set_split('train')\n",
    "        batch_generator = generate_batches(dataset, \n",
    "                                           batch_size=args.batch_size, \n",
    "                                           device=args.device)\n",
    "        running_loss = 0.0\n",
    "        running_acc = 0.0\n",
    "        classifier.train()\n",
    "\n",
    "        for batch_index, batch_dict in enumerate(batch_generator):\n",
    "            optimizer.zero_grad()  # 梯度清零\n",
    "            y_pred = classifier(batch_dict['x_surname'])  # 计算输出\n",
    "            loss = loss_func(y_pred, batch_dict['y_nationality'])  # 计算损失\n",
    "            loss_t = loss.item()\n",
    "            running_loss += (loss_t - running_loss) / (batch_index + 1)\n",
    "            loss.backward()  # 计算梯度\n",
    "            optimizer.step()  # 更新参数\n",
    "            acc_t = compute_accuracy(y_pred, batch_dict['y_nationality'])  # 计算准确率\n",
    "            running_acc += (acc_t - running_acc) / (batch_index + 1)\n",
    "\n",
    "            # 更新训练集进度条\n",
    "            train_bar.set_postfix(loss=running_loss, acc=running_acc, epoch=epoch_index)\n",
    "            train_bar.update()\n",
    "\n",
    "        train_state['train_loss'].append(running_loss)\n",
    "        train_state['train_acc'].append(running_acc)\n",
    "\n",
    "        # 迭代验证集\n",
    "        dataset.set_split('val')\n",
    "        batch_generator = generate_batches(dataset, \n",
    "                                           batch_size=args.batch_size, \n",
    "                                           device=args.device)\n",
    "        running_loss = 0.\n",
    "        running_acc = 0.\n",
    "        classifier.eval()\n",
    "\n",
    "        for batch_index, batch_dict in enumerate(batch_generator):\n",
    "            y_pred =  classifier(batch_dict['x_surname'])  # 计算输出\n",
    "            loss = loss_func(y_pred, batch_dict['y_nationality'])  # 计算损失\n",
    "            loss_t = loss.to(\"cpu\").item()\n",
    "            running_loss += (loss_t - running_loss) / (batch_index + 1)\n",
    "            acc_t = compute_accuracy(y_pred, batch_dict['y_nationality'])  # 计算准确率\n",
    "            running_acc += (acc_t - running_acc) / (batch_index + 1)\n",
    "\n",
    "            # 更新验证集进度条\n",
    "            val_bar.set_postfix(loss=running_loss, acc=running_acc, epoch=epoch_index)\n",
    "            val_bar.update()\n",
    "\n",
    "        train_state['val_loss'].append(running_loss)\n",
    "        train_state['val_acc'].append(running_acc)\n",
    "\n",
    "        # 更新训练状态\n",
    "        train_state = update_train_state(args=args, model=classifier, train_state=train_state)\n",
    "\n",
    "        scheduler.step(train_state['val_loss'][-1])  # 更新学习率\n",
    "\n",
    "        if train_state['stop_early']:  # 如果满足提前停止条件，跳出循环\n",
    "            break\n",
    "\n",
    "        train_bar.n = 0  # 重置训练集进度条\n",
    "        val_bar.n = 0  # 重置验证集进度条\n",
    "        epoch_bar.update()  # 更新总体进度条\n",
    "except KeyboardInterrupt:\n",
    "    print(\"Exiting loop\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ca81c1c5-979f-4162-b33f-9358ead2a5d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "最终训练集损失： 1.273\n",
      "最终训练集准确率： 51.76%\n",
      "最终验证集损失： 1.805\n",
      "最终验证集准确率： 45.44%\n"
     ]
    }
   ],
   "source": [
    "print(\"最终训练集损失： {:.3f}\".format(train_state['train_loss'][-1]))\n",
    "print(\"最终训练集准确率： {:.2f}%\".format(train_state['train_acc'][-1]))\n",
    "print(\"最终验证集损失： {:.3f}\".format(train_state['val_loss'][-1]))\n",
    "print(\"最终验证集准确率： {:.2f}%\".format(train_state['val_acc'][-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41186fc0-7093-4938-b6cd-19ed3df528c8",
   "metadata": {},
   "source": [
    "**2.3.2.3  EVALUATING ON THE TEST DATASET**   \n",
    "\n",
    "评价SurnameClassifier测试数据,我们将数据集设置为遍历测试数据,调用classifier.eval()方法,并遍历测试数据以同样的方式与其他数据。  \n",
    "调用classifier.eval()可以防止PyTorch在使用测试/评估数据时更新模型参数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5ba4ea3f-69a9-462d-8d2b-4aae4f5f3a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用最佳模型计算测试集上的损失和准确率\n",
    "classifier.load_state_dict(torch.load(train_state['model_filename']))  # 加载最佳模型参数\n",
    "\n",
    "classifier = classifier.to(args.device)  # 将模型移动到指定设备\n",
    "dataset.class_weights = dataset.class_weights.to(args.device)  # 将类别权重移动到指定设备\n",
    "loss_func = nn.CrossEntropyLoss(dataset.class_weights)  # 定义损失函数\n",
    "\n",
    "dataset.set_split('test')  # 设置数据集为测试集\n",
    "batch_generator = generate_batches(dataset, \n",
    "                                   batch_size=args.batch_size, \n",
    "                                   device=args.device)  # 创建批生成器\n",
    "running_loss = 0.\n",
    "running_acc = 0.\n",
    "classifier.eval()  # 设置模型为评估模式\n",
    "\n",
    "for batch_index, batch_dict in enumerate(batch_generator):\n",
    "    y_pred =  classifier(batch_dict['x_surname'])  # 计算模型输出\n",
    "    \n",
    "    loss = loss_func(y_pred, batch_dict['y_nationality'])  # 计算损失\n",
    "    loss_t = loss.item()\n",
    "    running_loss += (loss_t - running_loss) / (batch_index + 1)  # 更新损失值\n",
    "\n",
    "    acc_t = compute_accuracy(y_pred, batch_dict['y_nationality'])  # 计算准确率\n",
    "    running_acc += (acc_t - running_acc) / (batch_index + 1)  # 更新准确率\n",
    "\n",
    "# 保存测试集上的损失和准确率到训练状态中\n",
    "train_state['test_loss'] = running_loss\n",
    "train_state['test_acc'] = running_acc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fa58371-4927-40cf-a38c-bd96ca03a0f8",
   "metadata": {},
   "source": [
    "该模型对测试数据的准确性达到46%左右，根据前面的训练结果，会注意到在训练数据上的性能更高。这是因为模型总是更适合它所训练的数据，所以训练数据的性能并不代表新数据的性能。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cd25b9fe-2f06-43a2-a589-ec52043aac16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "测试集损失: 1.7831686401367184;\n",
      "测试集准确率: 46.31249999999999\n"
     ]
    }
   ],
   "source": [
    "print(\"测试集损失: {};\".format(train_state['test_loss']))\n",
    "print(\"测试集准确率: {}\".format(train_state['test_acc']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87560cb7-f73a-4fa9-af24-99a8867dfadf",
   "metadata": {},
   "source": [
    "你可以尝试隐藏维度的不同大小，应该注意到性能的提高。然而，这种增长不会很大，主要原因是收缩的onehot向量化方法是一种弱表示。  \n",
    "\n",
    "   \n",
    "压缩的one-hot向量化方法是一种将文本数据表示为向量的技术。  \n",
    "在这种方法中，文本中的每个字符被转换为一个固定长度的向量，通常是一个独热向量（one-hot vector）。独热向量是指在向量的维度中，只有一个元素为1，其他元素为0，用来表示字符的存在与否。\n",
    "举个例子，假设我们有一个包含26个字母的字母表（a到z），那么每个字母可以被表示为一个长度为26的独热向量。比如，字母\"a\"可以表示为[1, 0, 0, ..., 0]，字母\"b\"可以表示为[0, 1, 0, ..., 0]，以此类推。  \n",
    "\n",
    "  \n",
    "然而，这种方法存在一个局限性，即它丢失了字符在文本中的顺序信息。在自然语言处理任务中，字符的顺序通常包含了重要的语义信息。例如，在姓氏分类的任务中，不同的姓氏可能由相同的字符组成，但是字符的排列顺序可能是不同族群或文化的重要特征。  \n",
    "因此，尽管压缩的one-hot向量化方法简洁地将每个姓氏表示为单个向量，但它无法捕捉字符之间的顺序信息，这可能限制了模型对文本数据的理解能力。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84603248-a949-48e1-b50c-9d25ea2e19e6",
   "metadata": {},
   "source": [
    "**2.3.3  CLASSIFYING A NEW SURNAME**   \n",
    "现在，我们输入一个新的姓氏字符串，将其向量化后利用模型进行预测。   \n",
    "传入参数时apply_softmax=True，所以得到的结果是概率值。预测结果result是类概率的列表，使用PyTorch张量最大函数来得到由最高预测概率表示的最优类，即概率最大的国籍。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "20aa3c49-ffd0-4e9e-93c3-09e0184a1ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_nationality(surname, classifier, vectorizer):\n",
    "    \"\"\"预测一个新姓氏的国籍\n",
    "    \n",
    "    参数:\n",
    "        surname (str): 要分类的姓氏\n",
    "        classifier (SurnameClassifer): 分类器的实例\n",
    "        vectorizer (SurnameVectorizer): 对应的向量化器\n",
    "    返回:\n",
    "        包含最大概率的国籍及其概率的字典\n",
    "    \"\"\"\n",
    "    # 将姓氏向量化\n",
    "    vectorized_surname = vectorizer.vectorize(surname)\n",
    "    vectorized_surname = torch.tensor(vectorized_surname).view(1, -1)\n",
    "    \n",
    "    # 使用分类器进行预测\n",
    "    result = classifier(vectorized_surname, apply_softmax=True)\n",
    "    \n",
    "    # 获取概率最高的国籍及其概率值\n",
    "    probability_values, indices = result.max(dim=1)\n",
    "    index = indices.item()\n",
    "    predicted_nationality = vectorizer.nationality_vocab.lookup_index(index)\n",
    "    probability_value = probability_values.item()\n",
    "    \n",
    "    # 返回结果\n",
    "    return {'nationality': predicted_nationality, 'probability': probability_value}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ed131669-f60f-4ec1-a560-f48aa0e7dc9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "输入要分类的姓氏:  Alice\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alice -> Italian (p=0.73)\n"
     ]
    }
   ],
   "source": [
    "new_surname = input(\"输入要分类的姓氏: \")\n",
    "classifier = classifier.to(\"cpu\")\n",
    "prediction = predict_nationality(new_surname, classifier, vectorizer)\n",
    "print(\"{} -> {} (p={:0.2f})\".format(new_surname,\n",
    "                                    prediction['nationality'],\n",
    "                                    prediction['probability']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c926dd6-f019-4b63-8e5a-3635082fa267",
   "metadata": {},
   "source": [
    "我们不仅可以获得概率最高的一个预测，也可以获取更多的预测。  \n",
    "PyTorch提供了一个torch.topk函数,用于在指定维度上获取张量中最大的 k 个元素及其对应的索引,我们可以使用它来便利地获取更多预测。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f425615e-478f-4188-9c6a-116ef68c79b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "输入要分类的姓氏:  Alice\n",
      "要显示前几个预测结果？ 5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "前5个预测结果:\n",
      "===================\n",
      "Alice -> Italian (概率=0.73)\n",
      "Alice -> Spanish (概率=0.12)\n",
      "Alice -> Dutch (概率=0.03)\n",
      "Alice -> English (概率=0.02)\n",
      "Alice -> Czech (概率=0.02)\n"
     ]
    }
   ],
   "source": [
    "# 预测前k个可能的国籍\n",
    "def predict_topk_nationality(name, classifier, vectorizer, k=5):\n",
    "    # 向量化输入的名字\n",
    "    vectorized_name = vectorizer.vectorize(name)\n",
    "    vectorized_name = torch.tensor(vectorized_name).view(1, -1)\n",
    "    \n",
    "    # 使用分类器进行预测\n",
    "    prediction_vector = classifier(vectorized_name, apply_softmax=True)\n",
    "    \n",
    "    # 获取概率最高的前k个国籍及其概率值\n",
    "    probability_values, indices = torch.topk(prediction_vector, k=k)\n",
    "    \n",
    "    # 转换为numpy数组\n",
    "    probability_values = probability_values.detach().numpy()[0]\n",
    "    indices = indices.detach().numpy()[0]\n",
    "    \n",
    "    # 构建结果列表\n",
    "    results = []\n",
    "    for prob_value, index in zip(probability_values, indices):\n",
    "        nationality = vectorizer.nationality_vocab.lookup_index(index)\n",
    "        results.append({'nationality': nationality, 'probability': prob_value})\n",
    "    \n",
    "    return results\n",
    "\n",
    "# 输入要分类的新姓氏\n",
    "new_surname = input(\"输入要分类的姓氏: \")\n",
    "\n",
    "# 将分类器移至CPU上进行推断\n",
    "classifier = classifier.to(\"cpu\")\n",
    "\n",
    "# 询问要显示前k个预测结果\n",
    "k = int(input(\"要显示前几个预测结果？\"))\n",
    "if k > len(vectorizer.nationality_vocab):\n",
    "    print(\"抱歉！这超出了我们拥有的国籍数量... 默认显示最大数量的结果:)\")\n",
    "    k = len(vectorizer.nationality_vocab)\n",
    "\n",
    "# 进行预测\n",
    "predictions = predict_topk_nationality(new_surname, classifier, vectorizer, k=k)\n",
    "\n",
    "# 打印预测的结果\n",
    "print(\"前{}个预测结果:\".format(k))\n",
    "print(\"===================\")\n",
    "for prediction in predictions:\n",
    "    print(\"{} -> {} (概率={:0.2f})\".format(new_surname, prediction['nationality'], prediction['probability']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4a701b1-5a84-428a-9742-000e31c60a40",
   "metadata": {},
   "source": [
    "**2.3.4 DROPOUT**  \n",
    "Dropout 是一种在神经网络训练过程中常用的正则化技术，旨在减少过拟合并提高模型的泛化能力，我们也可以将它应用于MLPs。   \n",
    "  \n",
    "在传统的神经网络中，训练过程中每个神经元都会参与计算，这可能会导致神经元之间形成复杂的共适应关系，从而导致过拟合。Dropout 通过在训练过程中以一定概率随机地将部分神经元的输出置为0，从而减少神经元之间的依赖关系，使得网络更加健壮，减少过拟合的风险。\n",
    "\n",
    "具体来说，对于每个训练样本，Dropout 在前向传播过程中随机地将一部分神经元的输出置为0。这些被置为0的神经元在该次前向传播中不参与计算，而在反向传播时也不更新参数。在测试阶段，不再进行随机丢弃，而是将所有神经元的输出乘以保留概率（通常为1减去丢弃概率），以保持期望输出的一致性。\n",
    "\n",
    "Dropout 的主要优势在于它不依赖于特定的架构，因此可以应用于各种深度学习模型中，包括卷积神经网络、循环神经网络等。它在许多实际应用中都被证明能够改善模型的泛化能力，减少过拟合，从而提高模型的性能。  \n",
    "\n",
    "下面给出一个带dropout的MLP的重新实现。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "aac4b21e-4aaa-4138-b811-0575593519f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class MultilayerPerceptron(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        \"\"\"\n",
    "        初始化\n",
    "        \"\"\"\n",
    "        super(MultilayerPerceptron, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x_in, apply_softmax=False):\n",
    "        \"\"\"\n",
    "        MLP的前向传播\n",
    "        \"\"\"\n",
    "        intermediate = F.relu(self.fc1(x_in))\n",
    "        # 使用dropout进行正则化，减少过拟合\n",
    "        output = self.fc2(F.dropout(intermediate, p=0.5))\n",
    "\n",
    "        if apply_softmax:\n",
    "            output = F.softmax(output, dim=1)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b012ea58-b134-46b5-b570-e827554e1c41",
   "metadata": {},
   "source": [
    "# 三、Convolutional Neural Networks （卷积神经网络）\n",
    "\n",
    "卷积神经网络（Convolutional Neural Networks，CNNs）是一种在计算机视觉和图像识别领域广泛应用的深度学习模型。CNNs的设计灵感来源于生物视觉系统的工作原理，特别是人类视觉皮层的结构和功能。  \n",
    "CNNs的核心思想是利用卷积层（Convolutional Layer）和池化层（Pooling Layer）来自动提取图像中的特征，并通过全连接层（Fully Connected Layer）进行分类或预测。  \n",
    "\n",
    "以下是CNNs的主要组成部分：  \n",
    "  \n",
    "1.卷积层：  \n",
    "卷积层是CNNs的核心组件。它使用一组可学习的滤波器（也称为卷积核）来在输入图像上进行滑动窗口的卷积操作。每个滤波器在输入图像上提取特定的局部特征，例如边缘、纹理或形状。卷积操作通过计算滤波器与输入图像的对应位置的元素乘积的累加来生成特征图（Feature Map）。通过使用多个滤波器，卷积层可以提取不同的特征。  \n",
    "  \n",
    "2.池化层：  \n",
    "池化层用于减小特征图的空间尺寸，并减少参数数量。常见的池化操作包括最大池化（Max Pooling）和平均池化（Average Pooling）。池化操作在每个池化区域内取最大值或平均值作为输出，从而保留最显著的特征并减少计算量。池化层还具有一定的平移不变性，使得模型对输入图像的微小平移具有鲁棒性。  \n",
    "  \n",
    "3.全连接层：  \n",
    "全连接层是传统的神经网络层，其中每个神经元与前一层的所有神经元连接。在CNNs中，全连接层通常用于将卷积层和池化层提取的特征映射转换为最终的分类结果或预测结果。全连接层可以通过权重学习来建立输入特征和输出类别之间的关系。  \n",
    "\n",
    "除了以上的核心组件，CNNs还可以包括其他辅助组件，如批归一化（Batch Normalization）层用于加速训练过程和提高模型的鲁棒性，以及激活函数（Activation Function）层用于引入非线性性。\n",
    "\n",
    "通过多个卷积层、池化层和全连接层的堆叠，CNNs能够自动学习输入图像中的多层次抽象特征，并在分类、目标检测、图像分割等任务中取得出色的性能。CNNs的设计使其能够有效处理二维图像数据，但也可以应用于其他类型的数据，如音频和文本，通过适当的数据表示和卷积操作进行处理。  \n",
    "\n",
    "![](https://test.educg.net/userfiles/markdown/exp/2020_8/2034ll1597596025.png)\n",
    "<font color='grey'><center>图3-1 二维卷积运算。</center></font>\n",
    "\n",
    "![](https://p.sda1.dev/18/00a38a67520b7fa8ad8b8bb17af38688/296251fa81624246b56764208a2e6d58.png)\n",
    "<font color='grey'><center>图3-2 CNN文本分类模型结构图。</center></font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d617fb4e-5405-4783-8531-92205188f527",
   "metadata": {},
   "source": [
    "# 四、Classifying Surnames by Using a CNN （基于CNN的姓氏分类）   \n",
    "  \n",
    "下面我们将以一个具体的任务，来具体地说明如何使用CNN到一个分类任务。  \n",
    "基于CNN的文本分类和前面所使用的MLP，主要不同在于模型的构建和姓氏向量化的过程。  \n",
    "其中，使用one-hot矩阵作为CNN模型的输入，而不是前面示例中使用的压缩的one-hot编码。  \n",
    "这种设计可以更好地保留字符排列的信息，因为它能够更好地捕捉到输入数据的序列信息。  \n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96761ce0-fb6a-4f0d-b680-4467060bf479",
   "metadata": {},
   "source": [
    "**4.1 The SurnameDataset**  \n",
    "\n",
    "为此，我们需要实现一个数据集类，跟踪最长的姓氏，使用数据集中最长的姓氏来控制one-hot矩阵的大小。  \n",
    "这样做的原因是为了确保每个姓氏矩阵的大小相同，以便能够以相同的方式处理每个小批处理。  \n",
    "以下定义了这样的一个数据集类，这个类负责加载数据集、进行数据预处理、生成向量化的数据以及为模型提供适当的数据格式。  \n",
    "该类还包括了一些方法，用于在训练、验证和测试集之间进行切换，以及加载和保存向量化器。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fd586b9a-db5c-41e0-9612-e290c068a917",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class SurnameDataset(Dataset):\n",
    "    def __init__(self, surname_df, vectorizer):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            surname_df (pandas.DataFrame): 数据集\n",
    "            vectorizer (SurnameVectorizer): 从数据集实例化的向量化器\n",
    "        \"\"\"\n",
    "        # 初始化数据集\n",
    "        self.surname_df = surname_df\n",
    "        self._vectorizer = vectorizer\n",
    "        self.train_df = self.surname_df[self.surname_df.split=='train']\n",
    "        self.train_size = len(self.train_df)\n",
    "\n",
    "        self.val_df = self.surname_df[self.surname_df.split=='val']\n",
    "        self.validation_size = len(self.val_df)\n",
    "\n",
    "        self.test_df = self.surname_df[self.surname_df.split=='test']\n",
    "        self.test_size = len(self.test_df)\n",
    "\n",
    "        self._lookup_dict = {'train': (self.train_df, self.train_size),\n",
    "                             'val': (self.val_df, self.validation_size),\n",
    "                             'test': (self.test_df, self.test_size)}\n",
    "\n",
    "        self.set_split('train')\n",
    "        \n",
    "        # 类权重\n",
    "        class_counts = surname_df.nationality.value_counts().to_dict()\n",
    "        def sort_key(item):\n",
    "            return self._vectorizer.nationality_vocab.lookup_token(item[0])\n",
    "        sorted_counts = sorted(class_counts.items(), key=sort_key)\n",
    "        frequencies = [count for _, count in sorted_counts]\n",
    "        self.class_weights = 1.0 / torch.tensor(frequencies, dtype=torch.float32)\n",
    "\n",
    "\n",
    "    @classmethod\n",
    "    def load_dataset_and_make_vectorizer(cls, surname_csv):\n",
    "        \"\"\"加载数据集并从头开始创建一个新的向量化器\n",
    "        \n",
    "        Args:\n",
    "            surname_csv (str): 数据集的位置\n",
    "        Returns:\n",
    "            SurnameDataset的一个实例\n",
    "        \"\"\"\n",
    "        surname_df = pd.read_csv(surname_csv)\n",
    "        train_surname_df = surname_df[surname_df.split=='train']\n",
    "        return cls(surname_df, SurnameVectorizer.from_dataframe(train_surname_df))\n",
    "\n",
    "    @classmethod\n",
    "    def load_dataset_and_load_vectorizer(cls, surname_csv, vectorizer_filepath):\n",
    "        \"\"\"加载数据集和相应的向量化器。\n",
    "        用于在向量化器已经被缓存以便重复使用的情况下\n",
    "        \n",
    "        Args:\n",
    "            surname_csv (str): 数据集的位置\n",
    "            vectorizer_filepath (str): 保存的向量化器的位置\n",
    "        Returns:\n",
    "            SurnameDataset的一个实例\n",
    "        \"\"\"\n",
    "        surname_df = pd.read_csv(surname_csv)\n",
    "        vectorizer = cls.load_vectorizer_only(vectorizer_filepath)\n",
    "        return cls(surname_df, vectorizer)\n",
    "\n",
    "    @staticmethod\n",
    "    def load_vectorizer_only(vectorizer_filepath):\n",
    "        \"\"\"一个用于从文件加载向量化器的静态方法\n",
    "        \n",
    "        Args:\n",
    "            vectorizer_filepath (str): 序列化向量化器的位置\n",
    "        Returns:\n",
    "            SurnameDataset的一个实例\n",
    "        \"\"\"\n",
    "        with open(vectorizer_filepath) as fp:\n",
    "            return SurnameVectorizer.from_serializable(json.load(fp))\n",
    "\n",
    "    def save_vectorizer(self, vectorizer_filepath):\n",
    "        \"\"\"使用json将向量化器保存到磁盘\n",
    "        \n",
    "        Args:\n",
    "            vectorizer_filepath (str): 保存向量化器的位置\n",
    "        \"\"\"\n",
    "        with open(vectorizer_filepath, \"w\") as fp:\n",
    "            json.dump(self._vectorizer.to_serializable(), fp)\n",
    "\n",
    "    def get_vectorizer(self):\n",
    "        \"\"\"返回向量化器\"\"\"\n",
    "        return self._vectorizer\n",
    "\n",
    "    def set_split(self, split=\"train\"):\n",
    "        \"\"\"使用数据框中的列选择数据集的拆分\"\"\"\n",
    "        self._target_split = split\n",
    "        self._target_df, self._target_size = self._lookup_dict[split]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self._target_size\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"PyTorch数据集的主要入口方法\n",
    "        \n",
    "        Args:\n",
    "            index (int): 数据点的索引 \n",
    "        Returns:\n",
    "            一个包含数据点特征（x_data）和标签（y_target）的字典\n",
    "        \"\"\"\n",
    "        row = self._target_df.iloc[index]\n",
    "\n",
    "        surname_matrix = \\\n",
    "            self._vectorizer.vectorize(row.surname)\n",
    "\n",
    "        nationality_index = \\\n",
    "            self._vectorizer.nationality_vocab.lookup_token(row.nationality)\n",
    "\n",
    "        return {'x_surname': surname_matrix,\n",
    "                'y_nationality': nationality_index}\n",
    "\n",
    "    def get_num_batches(self, batch_size):\n",
    "        \"\"\"给定批次大小，返回数据集中的批次数\n",
    "        \n",
    "        Args:\n",
    "            batch_size (int)\n",
    "        Returns:\n",
    "            数据集中的批次数\n",
    "        \"\"\"\n",
    "        return len(self) // batch_size\n",
    "\n",
    "    \n",
    "def generate_batches(dataset, batch_size, shuffle=True,\n",
    "                     drop_last=True, device=\"cpu\"):\n",
    "    \"\"\"\n",
    "    一个生成器函数，它包装了PyTorch的DataLoader。它将确保每个张量位于正确的设备位置。\n",
    "    \"\"\"\n",
    "    dataloader = DataLoader(dataset=dataset, batch_size=batch_size,\n",
    "                            shuffle=shuffle, drop_last=drop_last)\n",
    "\n",
    "    for data_dict in dataloader:\n",
    "        out_data_dict = {}\n",
    "        for name, tensor in data_dict.items():\n",
    "            out_data_dict[name] = data_dict[name].to(device)\n",
    "        yield out_data_dict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dd1a65b-824d-4faa-a6c8-03e815f0bd0b",
   "metadata": {},
   "source": [
    "**4.2 Vocabulary, Vectorizer, and DataLoader**   \n",
    "为了适应CNN模型的需求，我们对Vectorizer的vectorize()方法进行了修改。  \n",
    "该方法将姓氏中的每个字符映射到一个整数，并构建一个由onehot向量组成的矩阵。在这个例子中，矩阵的每一列都是不同的onehot向量。  \n",
    "这种修改是为了适应使用Conv1d层的CNN模型的要求。Conv1d层期望数据张量在第0维上具有批处理维度，在第1维上具有通道维度，在第2维上具有特征维度。因此，我们使用onehot矩阵表示每个字符，并将它们作为不同的通道传递给CNN模型。\n",
    "\n",
    "另外，我们还对Vectorizer进行了一些其他修改，以计算姓氏的最大长度并将其保存为max_surname_length。这是为了在数据预处理阶段确定输入数据的维度，并在模型中使用适当的维度设置。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6cf08c86-4dba-4533-bd15-4beb8e5ca47c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SurnameVectorizer(object):\n",
    "    \"\"\"协调词汇表并将其应用的向量化器\"\"\"\n",
    "    def __init__(self, surname_vocab, nationality_vocab, max_surname_length):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            surname_vocab (Vocabulary): 将字符映射到整数的词汇表\n",
    "            nationality_vocab (Vocabulary): 将国籍映射到整数的词汇表\n",
    "            max_surname_length (int): 最长姓氏的长度\n",
    "        \"\"\"\n",
    "        self.surname_vocab = surname_vocab\n",
    "        self.nationality_vocab = nationality_vocab\n",
    "        self._max_surname_length = max_surname_length\n",
    "\n",
    "    def vectorize(self, surname):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            surname (str): 姓氏\n",
    "        Returns:\n",
    "            one_hot_matrix (np.ndarray): 一个独热向量矩阵\n",
    "        \"\"\"\n",
    "\n",
    "        one_hot_matrix_size = (len(self.surname_vocab), self._max_surname_length)\n",
    "        one_hot_matrix = np.zeros(one_hot_matrix_size, dtype=np.float32)\n",
    "                               \n",
    "        for position_index, character in enumerate(surname):\n",
    "            character_index = self.surname_vocab.lookup_token(character)\n",
    "            one_hot_matrix[character_index][position_index] = 1\n",
    "        \n",
    "        return one_hot_matrix\n",
    "\n",
    "    @classmethod\n",
    "    def from_dataframe(cls, surname_df):\n",
    "        \"\"\"从数据框实例化向量化器\n",
    "        \n",
    "        Args:\n",
    "            surname_df (pandas.DataFrame): 姓氏数据集\n",
    "        Returns:\n",
    "            SurnameVectorizer的一个实例\n",
    "        \"\"\"\n",
    "        surname_vocab = Vocabulary(unk_token=\"@\")\n",
    "        nationality_vocab = Vocabulary(add_unk=False)\n",
    "        max_surname_length = 0\n",
    "\n",
    "        for index, row in surname_df.iterrows():\n",
    "            max_surname_length = max(max_surname_length, len(row.surname))\n",
    "            for letter in row.surname:\n",
    "                surname_vocab.add_token(letter)\n",
    "            nationality_vocab.add_token(row.nationality)\n",
    "\n",
    "        return cls(surname_vocab, nationality_vocab, max_surname_length)\n",
    "\n",
    "    @classmethod\n",
    "    def from_serializable(cls, contents):\n",
    "        surname_vocab = Vocabulary.from_serializable(contents['surname_vocab'])\n",
    "        nationality_vocab =  Vocabulary.from_serializable(contents['nationality_vocab'])\n",
    "        return cls(surname_vocab=surname_vocab, nationality_vocab=nationality_vocab, \n",
    "                   max_surname_length=contents['max_surname_length'])\n",
    "\n",
    "    def to_serializable(self):\n",
    "        return {'surname_vocab': self.surname_vocab.to_serializable(),\n",
    "                'nationality_vocab': self.nationality_vocab.to_serializable(), \n",
    "                'max_surname_length': self._max_surname_length}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4f4baf5-445c-480a-91a3-910725731be1",
   "metadata": {},
   "source": [
    "接下来定义Vocabulary（词汇表）类。参考前面的MLP，结构基本一致。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "18a8d0f0-e5ab-4c19-b843-7d5b57521116",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocabulary(object):\n",
    "    \"\"\"处理文本并提取词汇表以进行映射的类\"\"\"\n",
    "\n",
    "    def __init__(self, token_to_idx=None, add_unk=True, unk_token=\"<UNK>\"):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            token_to_idx (dict): 一个预先存在的token到index的映射\n",
    "            add_unk (bool): 一个指示是否添加UNK token的标志\n",
    "            unk_token (str): 要添加到词汇表中的UNK token\n",
    "        \"\"\"\n",
    "\n",
    "        if token_to_idx is None:\n",
    "            token_to_idx = {}\n",
    "        self._token_to_idx = token_to_idx\n",
    "\n",
    "        self._idx_to_token = {idx: token \n",
    "                              for token, idx in self._token_to_idx.items()}\n",
    "        \n",
    "        self._add_unk = add_unk\n",
    "        self._unk_token = unk_token\n",
    "        \n",
    "        self.unk_index = -1\n",
    "        if add_unk:\n",
    "            self.unk_index = self.add_token(unk_token) \n",
    "        \n",
    "        \n",
    "    def to_serializable(self):\n",
    "        \"\"\"返回一个可以序列化的字典\"\"\"\n",
    "        return {'token_to_idx': self._token_to_idx, \n",
    "                'add_unk': self._add_unk, \n",
    "                'unk_token': self._unk_token}\n",
    "\n",
    "    @classmethod\n",
    "    def from_serializable(cls, contents):\n",
    "        \"\"\"从序列化的字典实例化Vocabulary\"\"\"\n",
    "        return cls(**contents)\n",
    "\n",
    "    def add_token(self, token):\n",
    "        \"\"\"基于token更新映射字典\n",
    "        \n",
    "        Args:\n",
    "            token (str): 要添加到词汇表中的项\n",
    "        Returns:\n",
    "            index (int): 对应于token的整数\n",
    "        \"\"\"\n",
    "        try:\n",
    "            index = self._token_to_idx[token]\n",
    "        except KeyError:\n",
    "            index = len(self._token_to_idx)\n",
    "            self._token_to_idx[token] = index\n",
    "            self._idx_to_token[index] = token\n",
    "        return index\n",
    "    \n",
    "    def add_many(self, tokens):\n",
    "        \"\"\"将token列表添加到词汇表中\n",
    "        \n",
    "        Args:\n",
    "            tokens (list): 字符串token列表\n",
    "        Returns:\n",
    "            indices (list): 与token对应的索引列表\n",
    "        \"\"\"\n",
    "        return [self.add_token(token) for token in tokens]\n",
    "\n",
    "    def lookup_token(self, token):\n",
    "        \"\"\"检索与token关联的索引或UNK索引（如果token不存在）\n",
    "        \n",
    "        Args:\n",
    "            token (str): 要查找的token\n",
    "        Returns:\n",
    "            index (int): 与token对应的索引\n",
    "        Notes:\n",
    "            UNK功能需要unk_index >= 0（已添加到词汇表中）\n",
    "        \"\"\"\n",
    "        if self.unk_index >= 0:\n",
    "            return self._token_to_idx.get(token, self.unk_index)\n",
    "        else:\n",
    "            return self._token_to_idx[token]\n",
    "\n",
    "    def lookup_index(self, index):\n",
    "        \"\"\"返回与索引关联的token\n",
    "        \n",
    "        Args: \n",
    "            index (int): 要查找的索引\n",
    "        Returns:\n",
    "            token (str): 与索引对应的token\n",
    "        Raises:\n",
    "            KeyError: 如果索引不在词汇表中\n",
    "        \"\"\"\n",
    "        if index not in self._idx_to_token:\n",
    "            raise KeyError(\"the index (%d) is not in the Vocabulary\" % index)\n",
    "        return self._idx_to_token[index]\n",
    "\n",
    "    def __str__(self):\n",
    "        return \"<Vocabulary(size=%d)>\" % len(self)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._token_to_idx)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "471af32f-d7db-486a-bba9-38b32f273afb",
   "metadata": {},
   "source": [
    "**4.3 Reimplementing the SurnameClassifier with Convolutional Networks**  \n",
    "在接下来我们所定义的姓氏分类器模型中，新增了sequence和ELU PyTorch模块。  \n",
    "sequence模块是封装线性操作序列的方便包装器。我们将它用于封装应用于Conv1d序列的操作。  \n",
    "而ELU是一种非线性函数，类似于ReLU，但不是将值裁剪到0以下，而是对它们求幂，对负值的处理更加平滑。  \n",
    "每个卷积的通道数与num_channels超参数绑定，这意味着可以根据需要选择不同数量的通道进行卷积运算。通过实验，发现使用256个通道能够使模型达到合理的性能。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2fe92cc2-c6e6-4757-a4ce-01879b3dcb8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SurnameClassifier(nn.Module):\n",
    "    def __init__(self, initial_num_channels, num_classes, num_channels):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            initial_num_channels (int): 输入特征向量的大小\n",
    "            num_classes (int): 输出预测向量的大小\n",
    "            num_channels (int): 网络中要使用的恒定通道大小\n",
    "        \"\"\"\n",
    "        super(SurnameClassifier, self).__init__()\n",
    "        \n",
    "        # 定义卷积神经网络的结构\n",
    "        self.convnet = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=initial_num_channels, \n",
    "                      out_channels=num_channels, kernel_size=3),  # 第一个卷积层\n",
    "            nn.ELU(),  # ELU非线性函数\n",
    "            nn.Conv1d(in_channels=num_channels, out_channels=num_channels, \n",
    "                      kernel_size=3, stride=2),  # 第二个卷积层\n",
    "            nn.ELU(),\n",
    "            nn.Conv1d(in_channels=num_channels, out_channels=num_channels, \n",
    "                      kernel_size=3, stride=2),  # 第三个卷积层\n",
    "            nn.ELU(),\n",
    "            nn.Conv1d(in_channels=num_channels, out_channels=num_channels, \n",
    "                      kernel_size=3),  # 第四个卷积层\n",
    "            nn.ELU()\n",
    "        )\n",
    "        \n",
    "        # 定义全连接层\n",
    "        self.fc = nn.Linear(num_channels, num_classes)\n",
    "\n",
    "    def forward(self, x_surname, apply_softmax=False):\n",
    "        \"\"\"分类器的前向传播\n",
    "        \n",
    "        Args:\n",
    "            x_surname (torch.Tensor): 输入数据张量。\n",
    "                x_surname.shape 应该是 (batch, initial_num_channels, max_surname_length)\n",
    "            apply_softmax (bool): 一个标志，用于指示是否应用softmax激活函数。\n",
    "                如果与交叉熵损失一起使用，应为False。\n",
    "        Returns:\n",
    "            结果张量。张量形状应该是 (batch, num_classes)\n",
    "        \"\"\"\n",
    "        # 执行卷积神经网络\n",
    "        features = self.convnet(x_surname).squeeze(dim=2)\n",
    "       \n",
    "        # 应用全连接层\n",
    "        prediction_vector = self.fc(features)\n",
    "\n",
    "        # 如果需要应用softmax激活函数\n",
    "        if apply_softmax:\n",
    "            prediction_vector = F.softmax(prediction_vector, dim=1)\n",
    "\n",
    "        return prediction_vector\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "807c68ad-9b2d-451a-8132-c6d45e60f4ca",
   "metadata": {},
   "source": [
    "**4.4 The Training Routine**  \n",
    "实例化数据集、模型、损失函数和优化器，然后遍历训练数据集来更新模型参数，接着遍历验证数据集来评估模型性能，并重复这个过程多次。这些步骤已经在MLP中展示过，只是输入参数发生了变化。，这里就不复述了。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4847435b-18b2-4845-b921-944ef672d275",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_train_state(args):\n",
    "    \"\"\"\n",
    "    创建训练状态的初始字典。\n",
    "\n",
    "    Args:\n",
    "        args: 参数对象，包含训练和模型的超参数信息。\n",
    "\n",
    "    Returns:\n",
    "        train_state: 包含训练状态信息的字典。\n",
    "    \"\"\"\n",
    "    return {'stop_early': False,\n",
    "            'early_stopping_step': 0,\n",
    "            'early_stopping_best_val': 1e8,\n",
    "            'learning_rate': args.learning_rate,\n",
    "            'epoch_index': 0,\n",
    "            'train_loss': [],\n",
    "            'train_acc': [],\n",
    "            'val_loss': [],\n",
    "            'val_acc': [],\n",
    "            'test_loss': -1,\n",
    "            'test_acc': -1,\n",
    "            'model_filename': args.model_state_file}\n",
    "\n",
    "\n",
    "def update_train_state(args, model, train_state):\n",
    "    \"\"\"\n",
    "    处理训练状态的更新。\n",
    "\n",
    "    组件:\n",
    "     - 提前停止: 防止过拟合。\n",
    "     - 模型检查点: 如果模型更好，则保存模型。\n",
    "\n",
    "    Args:\n",
    "        args: 主要参数\n",
    "        model: 要训练的模型\n",
    "        train_state: 表示训练状态值的字典\n",
    "\n",
    "    Returns:\n",
    "        train_state: 更新后的训练状态\n",
    "    \"\"\"\n",
    "\n",
    "    # 至少保存一个模型\n",
    "    if train_state['epoch_index'] == 0:\n",
    "        torch.save(model.state_dict(), train_state['model_filename'])\n",
    "        train_state['stop_early'] = False\n",
    "\n",
    "    # 如果训练过至少一个epoch\n",
    "    elif train_state['epoch_index'] >= 1:\n",
    "        loss_tm1, loss_t = train_state['val_loss'][-2:]\n",
    "\n",
    "        # 如果损失变大\n",
    "        if loss_t >= train_state['early_stopping_best_val']:\n",
    "            # 更新步数\n",
    "            train_state['early_stopping_step'] += 1\n",
    "        # 损失减小\n",
    "        else:\n",
    "            # 保存最佳模型\n",
    "            if loss_t < train_state['early_stopping_best_val']:\n",
    "                torch.save(model.state_dict(), train_state['model_filename'])\n",
    "\n",
    "            # 重置提前停止步数\n",
    "            train_state['early_stopping_step'] = 0\n",
    "\n",
    "        # 是否提前停止？\n",
    "        train_state['stop_early'] = \\\n",
    "            train_state['early_stopping_step'] >= args.early_stopping_criteria\n",
    "\n",
    "    return train_state\n",
    "\n",
    "\n",
    "def compute_accuracy(y_pred, y_target):\n",
    "    \"\"\"\n",
    "    计算预测的准确率。\n",
    "\n",
    "    Args:\n",
    "        y_pred: 模型的预测结果\n",
    "        y_target: 真实的目标值\n",
    "\n",
    "    Returns:\n",
    "        accuracy: 预测的准确率\n",
    "    \"\"\"\n",
    "    y_pred_indices = y_pred.max(dim=1)[1]\n",
    "    n_correct = torch.eq(y_pred_indices, y_target).sum().item()\n",
    "    return n_correct / len(y_pred_indices) * 100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8ad913a4-d81b-4719-8b91-6447a0a008fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expanded filepaths: \n",
      "\tmodel_storage/ch4/cnn\\vectorizer.json\n",
      "\tmodel_storage/ch4/cnn\\model.pth\n",
      "Using CUDA: False\n"
     ]
    }
   ],
   "source": [
    "from argparse import Namespace\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "args = Namespace(\n",
    "    # 数据和路径信息\n",
    "    surname_csv=\"surnames_with_splits.csv\",\n",
    "    vectorizer_file=\"vectorizer.json\",\n",
    "    model_state_file=\"model.pth\",\n",
    "    save_dir=\"model_storage/ch4/cnn\",\n",
    "    # 模型超参数\n",
    "    hidden_dim=100,\n",
    "    num_channels=256,\n",
    "    # 训练超参数\n",
    "    seed=1337,\n",
    "    learning_rate=0.001,\n",
    "    batch_size=128,\n",
    "    num_epochs=100,\n",
    "    early_stopping_criteria=5,\n",
    "    dropout_p=0.1,\n",
    "    # 运行时选项\n",
    "    cuda=False,\n",
    "    reload_from_files=False,\n",
    "    expand_filepaths_to_save_dir=True,\n",
    "    catch_keyboard_interrupt=True\n",
    ")\n",
    "\n",
    "# 如果expand_filepaths_to_save_dir为True，将文件路径扩展到保存目录\n",
    "if args.expand_filepaths_to_save_dir:\n",
    "    args.vectorizer_file = os.path.join(args.save_dir, args.vectorizer_file)\n",
    "    args.model_state_file = os.path.join(args.save_dir, args.model_state_file)\n",
    "    print(\"Expanded filepaths: \")\n",
    "    print(\"\\t{}\".format(args.vectorizer_file))\n",
    "    print(\"\\t{}\".format(args.model_state_file))\n",
    "\n",
    "# 检查CUDA是否可用\n",
    "if not torch.cuda.is_available():\n",
    "    args.cuda = False\n",
    "\n",
    "# 设置设备为cuda或cpu\n",
    "args.device = torch.device(\"cuda\" if args.cuda else \"cpu\")\n",
    "print(\"Using CUDA: {}\".format(args.cuda))\n",
    "\n",
    "def set_seed_everywhere(seed, cuda):\n",
    "    \"\"\"\n",
    "    设置随机种子以实现全局可重现性。\n",
    "\n",
    "    Args:\n",
    "        seed: 随机种子\n",
    "        cuda: 是否使用cuda\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if cuda:\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "def handle_dirs(dirpath):\n",
    "    \"\"\"\n",
    "    处理目录，如果目录不存在，则创建目录。\n",
    "\n",
    "    Args:\n",
    "        dirpath: 目录路径\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    if not os.path.exists(dirpath):\n",
    "        os.makedirs(dirpath)\n",
    "\n",
    "# 设置随机种子以实现可重现性\n",
    "set_seed_everywhere(args.seed, args.cuda)\n",
    "\n",
    "# 处理目录\n",
    "handle_dirs(args.save_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "811b3682-6026-4fea-957e-b59165dbb97b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\31216\\AppData\\Local\\Temp\\ipykernel_25992\\747011388.py:33: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  epoch_bar = tqdm_notebook(desc='training routine', total=args.num_epochs, position=0)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38bd013145614e5d81d659281f90bb7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "training routine:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\31216\\AppData\\Local\\Temp\\ipykernel_25992\\747011388.py:35: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  train_bar = tqdm_notebook(desc='split=train', total=dataset.get_num_batches(args.batch_size), position=1, leave=True)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d205b2c57bf7438c85301b7e59419034",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "split=train:   0%|          | 0/60 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\31216\\AppData\\Local\\Temp\\ipykernel_25992\\747011388.py:37: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  val_bar = tqdm_notebook(desc='split=val', total=dataset.get_num_batches(args.batch_size), position=1, leave=True)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41b061bf328f498baf504ac7049d2752",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "split=val:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "if args.reload_from_files:\n",
    "    # 从检查点训练\n",
    "    dataset = SurnameDataset.load_dataset_and_load_vectorizer(args.surname_csv, args.vectorizer_file)\n",
    "else:\n",
    "    # 创建数据集和矢量化器\n",
    "    dataset = SurnameDataset.load_dataset_and_make_vectorizer(args.surname_csv)\n",
    "    dataset.save_vectorizer(args.vectorizer_file)\n",
    "\n",
    "vectorizer = dataset.get_vectorizer()\n",
    "\n",
    "classifier = SurnameClassifier(initial_num_channels=len(vectorizer.surname_vocab), \n",
    "                               num_classes=len(vectorizer.nationality_vocab),\n",
    "                               num_channels=args.num_channels)\n",
    "\n",
    "# 将分类器移动到指定的设备上\n",
    "classifier = classifier.to(args.device)\n",
    "dataset.class_weights = dataset.class_weights.to(args.device)\n",
    "\n",
    "loss_func = nn.CrossEntropyLoss(weight=dataset.class_weights)\n",
    "optimizer = optim.Adam(classifier.parameters(), lr=args.learning_rate)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer=optimizer,\n",
    "                                                 mode='min', factor=0.5,\n",
    "                                                 patience=1)\n",
    "\n",
    "train_state = make_train_state(args)\n",
    "\n",
    "# 创建进度条\n",
    "epoch_bar = tqdm_notebook(desc='training routine', total=args.num_epochs, position=0)\n",
    "dataset.set_split('train')\n",
    "train_bar = tqdm_notebook(desc='split=train', total=dataset.get_num_batches(args.batch_size), position=1, leave=True)\n",
    "dataset.set_split('val')\n",
    "val_bar = tqdm_notebook(desc='split=val', total=dataset.get_num_batches(args.batch_size), position=1, leave=True)\n",
    "\n",
    "try:\n",
    "    for epoch_index in range(args.num_epochs):\n",
    "        train_state['epoch_index'] = epoch_index\n",
    "\n",
    "        # 遍历训练数据集\n",
    "\n",
    "        # 设置：批生成器，将损失和准确度设置为0，将训练模式打开\n",
    "        dataset.set_split('train')\n",
    "        batch_generator = generate_batches(dataset, batch_size=args.batch_size, device=args.device)\n",
    "        running_loss = 0.0\n",
    "        running_acc = 0.0\n",
    "        classifier.train()\n",
    "\n",
    "        for batch_index, batch_dict in enumerate(batch_generator):\n",
    "            # 训练流程的5个步骤：\n",
    "\n",
    "            # --------------------------------------\n",
    "            # 步骤1. 梯度清零\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # 步骤2. 计算输出\n",
    "            y_pred = classifier(batch_dict['x_surname'])\n",
    "\n",
    "            # 步骤3. 计算损失\n",
    "            loss = loss_func(y_pred, batch_dict['y_nationality'])\n",
    "            loss_t = loss.item()\n",
    "            running_loss += (loss_t - running_loss) / (batch_index + 1)\n",
    "\n",
    "            # 步骤4. 使用损失计算梯度\n",
    "            loss.backward()\n",
    "\n",
    "            # 步骤5. 使用优化器进行梯度更新\n",
    "            optimizer.step()\n",
    "            # -----------------------------------------\n",
    "            # 计算准确度\n",
    "            acc_t = compute_accuracy(y_pred, batch_dict['y_nationality'])\n",
    "            running_acc += (acc_t - running_acc) / (batch_index + 1)\n",
    "\n",
    "            # 更新进度条\n",
    "            train_bar.set_postfix(loss=running_loss, acc=running_acc, epoch=epoch_index)\n",
    "            train_bar.update()\n",
    "\n",
    "        train_state['train_loss'].append(running_loss)\n",
    "        train_state['train_acc'].append(running_acc)\n",
    "\n",
    "        # 遍历验证数据集\n",
    "\n",
    "        # 设置：批生成器，将损失和准确度设置为0，将评估模式打开\n",
    "        dataset.set_split('val')\n",
    "        batch_generator = generate_batches(dataset, batch_size=args.batch_size, device=args.device)\n",
    "        running_loss = 0.\n",
    "        running_acc = 0.\n",
    "        classifier.eval()\n",
    "\n",
    "        for batch_index, batch_dict in enumerate(batch_generator):\n",
    "\n",
    "            # 计算输出\n",
    "            y_pred = classifier(batch_dict['x_surname'])\n",
    "\n",
    "            # 步骤3. 计算损失\n",
    "            loss = loss_func(y_pred, batch_dict['y_nationality'])\n",
    "            loss_t = loss.item()\n",
    "            running_loss += (loss_t - running_loss) / (batch_index + 1)\n",
    "\n",
    "            # 计算准确度\n",
    "            acc_t = compute_accuracy(y_pred, batch_dict['y_nationality'])\n",
    "            running_acc += (acc_t - running_acc) / (batch_index + 1)\n",
    "            val_bar.set_postfix(loss=running_loss, acc=running_acc, epoch=epoch_index)\n",
    "            val_bar.update()\n",
    "\n",
    "        train_state['val_loss'].append(running_loss)\n",
    "        train_state['val_acc'].append(running_acc)\n",
    "\n",
    "        # 更新训练状态\n",
    "        train_state = update_train_state(args=args, model=classifier, train_state=train_state)\n",
    "\n",
    "        # 更新学习率\n",
    "        scheduler.step(train_state['val_loss'][-1])\n",
    "\n",
    "        if train_state['stop_early']:\n",
    "            break\n",
    "\n",
    "        train_bar.n = 0\n",
    "        val_bar.n = 0\n",
    "        epoch_bar.update()\n",
    "except KeyboardInterrupt:\n",
    "    print(\"Exiting loop\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "85eb45ab-aeb4-45ff-9102-4dc6dc89f70d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "最终训练集损失： 0.673\n",
      "最终训练集准确率： 68.26%\n",
      "最终验证集损失： 2.052\n",
      "最终验证集准确率： 56.25%\n"
     ]
    }
   ],
   "source": [
    "print(\"最终训练集损失： {:.3f}\".format(train_state['train_loss'][-1]))\n",
    "print(\"最终训练集准确率： {:.2f}%\".format(train_state['train_acc'][-1]))\n",
    "print(\"最终验证集损失： {:.3f}\".format(train_state['val_loss'][-1]))\n",
    "print(\"最终验证集准确率： {:.2f}%\".format(train_state['val_acc'][-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4d75ec04-0266-4843-bdc3-fa3e6be0da90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载模型状态字典\n",
    "classifier.load_state_dict(torch.load(train_state['model_filename']))\n",
    "\n",
    "# 将分类器移动到指定的设备上\n",
    "classifier = classifier.to(args.device)\n",
    "dataset.class_weights = dataset.class_weights.to(args.device)\n",
    "\n",
    "# 定义损失函数\n",
    "loss_func = nn.CrossEntropyLoss(dataset.class_weights)\n",
    "\n",
    "# 设置数据集为测试集\n",
    "dataset.set_split('test')\n",
    "\n",
    "# 创建测试集的批生成器\n",
    "batch_generator = generate_batches(dataset, \n",
    "                                   batch_size=args.batch_size, \n",
    "                                   device=args.device)\n",
    "running_loss = 0.\n",
    "running_acc = 0.\n",
    "classifier.eval()\n",
    "\n",
    "# 遍历测试集的批次\n",
    "for batch_index, batch_dict in enumerate(batch_generator):\n",
    "    # 计算输出\n",
    "    y_pred = classifier(batch_dict['x_surname'])\n",
    "    \n",
    "    # 计算损失\n",
    "    loss = loss_func(y_pred, batch_dict['y_nationality'])\n",
    "    loss_t = loss.item()\n",
    "    running_loss += (loss_t - running_loss) / (batch_index + 1)\n",
    "\n",
    "    # 计算准确度\n",
    "    acc_t = compute_accuracy(y_pred, batch_dict['y_nationality'])\n",
    "    running_acc += (acc_t - running_acc) / (batch_index + 1)\n",
    "\n",
    "# 记录测试集的损失和准确度\n",
    "train_state['test_loss'] = running_loss\n",
    "train_state['test_acc'] = running_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "75d45884-4cd9-4537-a009-81193ed72dfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "测试集损失: 1.8183989624182386;\n",
      "测试集准确率: 56.77083333333333\n"
     ]
    }
   ],
   "source": [
    "# 打印测试结果\n",
    "print(\"测试集损失: {};\".format(train_state['test_loss']))\n",
    "print(\"测试集准确率: {}\".format(train_state['test_acc']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "256df467-1313-4545-aed6-2c53fd3cab5a",
   "metadata": {},
   "source": [
    "执行评估的代码也没有变化，依然调用分类器的eval()方法来防止反向传播更新参数，并迭代测试数据集。与 MLP 约 46% 的性能相比，该模型的测试集性能准确率约为58%，可以看出我们在当前数据集上通过CNN得到了一个更好的评价结果，尽管这些性能数字绝不是这些特定架构的上限，但这个结果表明，在处理文本数据时，使用CNN模型是一个值得尝试的选择，它已经展示了CNN在文本分类任务上的潜力。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6001fda-7559-469a-9896-e7d124d22cf3",
   "metadata": {},
   "source": [
    "**4.5 Using the trained model to make predictions**  \n",
    "同样地，我们可以像MLP那样使用模型进行预测。  \n",
    "但有所不同的是，predict_nationality()函数的一部分发生了更改，通过使用PyTorch的unsqueeze()函数来添加大小为1的维度，而不是使用视图方法来重塑新创建的数据张量以添加批处理维度。  \n",
    "这种更改的目的是为了在张量的特定位置添加批处理维度，确保数据在进行预测时具有正确的形状和维度，符合模型的输入要求。  \n",
    "相同的更改反映在predict_topk_nationality()函数中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "48c0adc5-fac6-4d5b-910e-9cbcbf4725cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_nationality(surname, classifier, vectorizer):\n",
    "    \"\"\"根据姓氏预测国籍\n",
    "    \n",
    "    Args:\n",
    "        surname (str): 要分类的姓氏\n",
    "        classifier (SurnameClassifer): 分类器的实例\n",
    "        vectorizer (SurnameVectorizer): 对应的向量化器\n",
    "    \n",
    "    Returns:\n",
    "        dict: 包含最可能的国籍及其概率的字典\n",
    "    \"\"\"\n",
    "    # 将姓氏转换为向量表示\n",
    "    vectorized_surname = vectorizer.vectorize(surname)\n",
    "    # 添加批处理维度并转换为PyTorch张量\n",
    "    vectorized_surname = torch.tensor(vectorized_surname).unsqueeze(0)\n",
    "    # 使用分类器进行预测\n",
    "    result = classifier(vectorized_surname, apply_softmax=True)\n",
    "\n",
    "    # 获取最高概率和对应的索引\n",
    "    probability_values, indices = result.max(dim=1)\n",
    "    index = indices.item()\n",
    "\n",
    "    # 根据索引查找预测的国籍\n",
    "    predicted_nationality = vectorizer.nationality_vocab.lookup_index(index)\n",
    "    probability_value = probability_values.item()\n",
    "\n",
    "    # 返回预测的国籍和概率\n",
    "    return {'nationality': predicted_nationality, 'probability': probability_value}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4f633400-40f0-4c4f-b855-8cee02a750c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "请输入要分类的姓氏： Alice\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alice -> Italian (p=0.57)\n"
     ]
    }
   ],
   "source": [
    "# 获取用户输入的姓氏\n",
    "new_surname = input(\"请输入要分类的姓氏：\")\n",
    "# 将分类器移动到CPU上进行预测\n",
    "classifier = classifier.cpu()\n",
    "# 进行姓氏分类预测\n",
    "prediction = predict_nationality(new_surname, classifier, vectorizer)\n",
    "# 打印预测结果\n",
    "print(\"{} -> {} (p={:0.2f})\".format(new_surname,\n",
    "                                    prediction['nationality'],\n",
    "                                    prediction['probability']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5f1333bc-cc51-460b-8b8c-471d9b1a6140",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "输入要分类的姓氏： Alice\n",
      "要显示前几个预测结果？ 5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "前5个预测结果:\n",
      "===================\n",
      "Alice -> Italian (p=0.57)\n",
      "Alice -> Spanish (p=0.25)\n",
      "Alice -> English (p=0.05)\n",
      "Alice -> French (p=0.04)\n",
      "Alice -> Czech (p=0.04)\n"
     ]
    }
   ],
   "source": [
    "def predict_topk_nationality(surname, classifier, vectorizer, k=5):\n",
    "    \"\"\"预测给定姓氏的前K个国籍\n",
    "    \n",
    "    Args:\n",
    "        surname (str): 要分类的姓氏\n",
    "        classifier (SurnameClassifer): 分类器的实例\n",
    "        vectorizer (SurnameVectorizer): 对应的向量化器\n",
    "        k (int): 要返回的前K个国籍数量\n",
    "    \n",
    "    Returns:\n",
    "        list of dictionaries: 每个字典包含一个国籍和一个概率\n",
    "    \"\"\"\n",
    "    \n",
    "    # 将姓氏转换为向量表示\n",
    "    vectorized_surname = vectorizer.vectorize(surname)\n",
    "    vectorized_surname = torch.tensor(vectorized_surname).unsqueeze(dim=0)\n",
    "    # 使用分类器进行预测\n",
    "    prediction_vector = classifier(vectorized_surname, apply_softmax=True)\n",
    "    # 获取前K个最高概率的国籍\n",
    "    probability_values, indices = torch.topk(prediction_vector, k=k)\n",
    "    \n",
    "    # 转换为numpy数组\n",
    "    probability_values = probability_values[0].detach().numpy()\n",
    "    indices = indices[0].detach().numpy()\n",
    "    \n",
    "    results = []\n",
    "    for kth_index in range(k):\n",
    "        # 查找国籍并概率\n",
    "        nationality = vectorizer.nationality_vocab.lookup_index(indices[kth_index])\n",
    "        probability_value = probability_values[kth_index]\n",
    "        results.append({'nationality': nationality, \n",
    "                        'probability': probability_value})\n",
    "    return results\n",
    "\n",
    "new_surname = input(\"输入要分类的姓氏：\")\n",
    "\n",
    "k = int(input(\"要显示前几个预测结果？\"))\n",
    "if k > len(vectorizer.nationality_vocab):\n",
    "    print(\"抱歉！这超过了我们拥有的国籍数量...默认显示最大数量的结果:)\")\n",
    "    k = len(vectorizer.nationality_vocab)\n",
    "    \n",
    "predictions = predict_topk_nationality(new_surname, classifier, vectorizer, k=k)\n",
    "\n",
    "print(\"前{}个预测结果:\".format(k))\n",
    "print(\"===================\")\n",
    "for prediction in predictions:\n",
    "    print(\"{} -> {} (p={:0.2f})\".format(new_surname,\n",
    "                                        prediction['nationality'],\n",
    "                                        prediction['probability']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7ca011b-0f5a-4d4b-8d22-8c7dde47eba6",
   "metadata": {},
   "source": [
    "**4.6  Miscellaneous Topics in CNNs**   \n",
    "当讨论卷积神经网络（CNNs）的核心概念时，以下几个主题在它们的共同使用中起着重要作用：池化操作（Pooling）、批归一化（Batch Normalization）、网络内网络连接（Network-in-Network Connection）和残差连接（Residual Connections）。\n",
    "\n",
    "1. 池化操作（Pooling）：池化操作用于减小特征图的空间尺寸，同时保留重要的特征。常见的池化操作包括最大池化（Max Pooling）和平均池化（Average Pooling）。池化操作有助于减少模型的参数数量，提高模型的计算效率，并增强模型对平移和缩放的不变性。\n",
    "\n",
    "2. 批归一化（Batch Normalization）：批归一化用于加速神经网络的训练过程，并提高模型的鲁棒性。它通过对每个批次的输入进行归一化，使得网络的每一层的输入具有相似的分布。批归一化有助于解决梯度消失和梯度爆炸的问题，同时提高模型的泛化能力。\n",
    "\n",
    "3. 网络内网络连接（Network-in-Network Connection）：网络内网络连接是一种结构设计，用于增强模型的表示能力。它通过在卷积层中引入具有自己的权重和非线性激活函数的小型神经网络，来捕捉更复杂的特征。网络内网络连接可以增加模型的非线性能力，并提高模型对复杂数据模式的建模能力。\n",
    "\n",
    "4. 残差连接（Residual Connections）：残差连接是一种跳跃连接（Skip Connection）的形式，用于解决深层神经网络训练中的梯度消失和模型退化问题。通过将输入直接添加到网络的输出中，残差连接允许信息在网络中直接传递，避免了信息的丢失和损失。残差连接有助于训练更深的网络，并提高模型的性能和收敛速度。\n",
    "\n",
    "这些概念在CNNs的设计和训练中起着重要的作用，并帮助提高模型的性能和泛化能力。理解这些概念的原理和应用可以帮助您更好地设计和优化卷积神经网络模型。  \n",
    "由于关于CNN的相关资料在网上非常多，在最后我们就不再赘述。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
